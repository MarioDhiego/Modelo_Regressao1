[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Técnicas de Machine Learning no R",
    "section": "",
    "text": "Prefácio\nUm dos mais relevantes desafios, no atual cenário competitivo, refere-se à efetiva aplicação das técnicas analíticas no equacionamento e resolução dos problemas empresariais. O desenvolvimento de modelos de análises aplicados à realidade empresarial tornou-se uma competência crítica em todos os setores de atividade econômica. Esta habilidade é imperativa na esfera governamental e nas organizações sem finalidades lucrativas.\nO profissionais que atuam nos projetos de natureza analítica enfrentam desafios peculiares. Em geral, as organizações demandam soluções práticas e de rápida implantação. Por outro lado, existe um ritmo intenso de geração de novas Frameworks, Bibliotecas e ferramentas de software bem como do aperfeiçoamento contínuo dos algoritmos de análise.\nAs técnicas analíticas aqui expostas são aquelas que devem fazer parte do repertório obrigatório da Análise de Dados.\nEste livro foi escrito com objetivo de permitir que estudantes e pesquisadores entendam e apliquem diferentes algoritmos de Machine Learning, sem apronfundar-se nas teorias que os fundamentam. Preferiu-se apresentar os principais conceitos de forma intuitiva, para que o usuário compreenda a lógica de cada algoritmo, para que finalidade deve ser utilizado, quais os passos a seguir para aplicá-lo corretamente utilizando o sotware R e suas vantagens e limitações quando comparado com outros algritmos que podem ser utilizados no mesmo problema.",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "exploratoria.html",
    "href": "exploratoria.html",
    "title": "1  Análise Exploratória de Dados (EDA)",
    "section": "",
    "text": "1.1 Diretório de Trabalho\nEm estatística, a análise exploratória de dados (AED) é uma abordagem à análise de conjuntos de dados de modo a resumir suas características principais, frequentemente com métodos visuais. Um modelo estatístico pode ou não ser usado, mas primariamente a AED tem como objetivo observar o que os dados podem nos dizer além da modelagem formal ou do processo de teste de hipóteses.\nA análise exploratória de dados foi promovida pelo estatístico norte-americano John Tukey, que incentivava os estatísticos a explorar os dados e possivelmente formular hipóteses que poderiam levar a novas coletas de dados e experimentos.\nPrimeiro Passo é definir o diretório de trabalho no R\nsetwd('C:/Users/usuario/Documents/Modelo_Regressao')",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Análise Exploratória de Dados (EDA)</span>"
    ]
  },
  {
    "objectID": "exploratoria.html#carregar-dataset",
    "href": "exploratoria.html#carregar-dataset",
    "title": "1  Análise Exploratória de Dados (EDA)",
    "section": "1.2 Carregar Dataset",
    "text": "1.2 Carregar Dataset\nSegundo Passo é carregar a base de dados, chamada de mercado. Para isso é necessário instalar o pacote para leitura de arquivo com extensão do tipo excel(.xlsx) por meio do comando install.packages(“readxl”).\nPosteriormente, ativar o pacote no R com o comando library(readxl). Tendo um detalhe fundamental, que se instala somente um vez o pacote, e se ativa toda vez que for usar.\n\nlibrary(readxl)\n\nmercado &lt;- read_excel('mercado.xlsx')\nmercado2 &lt;- read_excel('mercado2.xlsx')\n\nA importância da apresentação dos dados é fundamental no início da faxina dos dados.\nPara a apresentação dos dataset contamos com alguns pacotes na linguagem R que possibilitam a apresentação de tabelas de maneira bastante satisfatória (de forma elegante e até interativa).\nO pacote DT é uma excelente opção quando se trata de uma apresentação rápida, geral e dinâmica sobre a base de dados.\nO pacote DT fornece uma interface R para a biblioteca JavaScript DataTables. Objetos de dados R (matrizes ou quadros de dados) podem ser exibidos como tabelas em páginas HTML, e DataTables fornece filtragem, paginação, classificação e muitos outros recursos nas tabelas.\nSegue a base de dados (n=80) referentes as características dos Funcionários que trabalham no Supermercado Formosa, na cidade de Belém, Estado do Pará, em 2023.\n\nlibrary(DT)\n\ndatatable(mercado,\n          class = 'cell-border stripe',\n          editable = 'cell',\n          caption = 'Tabela 01: Banco de Dados sobre Funcionários do Supermercado Formosa, Belém - Pará, 2023.')",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Análise Exploratória de Dados (EDA)</span>"
    ]
  },
  {
    "objectID": "exploratoria.html#identificar-os-tipos-de-variáveis",
    "href": "exploratoria.html#identificar-os-tipos-de-variáveis",
    "title": "1  Análise Exploratória de Dados (EDA)",
    "section": "1.3 Identificar os Tipos de Variáveis",
    "text": "1.3 Identificar os Tipos de Variáveis\nUtilizamos a função diagnose(), do paocte dlookr na linguagem R, para identificar os tipos de variáveis para análise.\nA função diagnose() da biblioteca dlookr que retorna por variável qual o tipo dela, contagem de valores faltantes, frequência de faltantes em relação à base toda.\n\nlibrary(dlookr)\n\nmercado %&gt;% dlookr::diagnose()\n\n# A tibble: 6 × 6\n  variables types     missing_count missing_percent unique_count unique_rate\n  &lt;chr&gt;     &lt;chr&gt;             &lt;int&gt;           &lt;dbl&gt;        &lt;int&gt;       &lt;dbl&gt;\n1 EDUCAÇÃO  character             0               0            2       0.025\n2 CARGO     character             0               0            4       0.05 \n3 LOCAL     character             0               0            2       0.025\n4 IDADE     numeric               0               0           36       0.45 \n5 TEMPOCASA numeric               0               0           21       0.262\n6 SALARIO   numeric               0               0           60       0.75",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Análise Exploratória de Dados (EDA)</span>"
    ]
  },
  {
    "objectID": "exploratoria.html#variáveis-qualitativas",
    "href": "exploratoria.html#variáveis-qualitativas",
    "title": "1  Análise Exploratória de Dados (EDA)",
    "section": "1.4 Variáveis Qualitativas",
    "text": "1.4 Variáveis Qualitativas\n\n1.4.1 Tabelas de Frequência: Simples\n\ntable(mercado2$CARGO)\n\n\nAUXILIAR  DIRETOR  GERENTE \n      37       12       30 \n\ntable(mercado2$EDUCAÇÃO)\n\n\nSECUNDÁRIO   SUPERIOR \n        15         64 \n\ntable(mercado2$LOCAL)\n\n\n CAPITAL INTERIOR \n      44       35 \n\n\n\n\n1.4.2 Tabelas de Frequência: Proporção\n\nprop.table(table(mercado2$CARGO))*100\n\n\nAUXILIAR  DIRETOR  GERENTE \n46.83544 15.18987 37.97468 \n\nprop.table(table(mercado2$EDUCAÇÃO))*100\n\n\nSECUNDÁRIO   SUPERIOR \n  18.98734   81.01266 \n\nprop.table(table(mercado2$LOCAL))*100\n\n\n CAPITAL INTERIOR \n 55.6962  44.3038 \n\n\n\n\n1.4.3 Tabelas de Contigência\nO pacote gtsummary fornece uma maneira elegante e flexível de criar tabelas analíticas e de resumo prontas para publicação usando a linguagem de programação R.\nA função tbl_summary() calcula estatísticas descritivas para variáveis contínuas, categóricas e dicotômicas em R e apresenta os resultados em uma tabela de resumo bonita e personalizável, pronta para publicação.\n\nlibrary(dplyr)\nlibrary(gtsummary)\n\nmercado %&gt;% \n  select( \n    LOCAL,\n    CARGO,\n    IDADE,\n    TEMPOCASA,\n    EDUCAÇÃO,\n    SALARIO) %&gt;% \n  tbl_summary(by = LOCAL,\n              statistic = list(all_continuous() ~ \"{mean} ({sd})\")) %&gt;%\n  modify_header(label = \"**VARIAVEIS**\") %&gt;%\n  modify_caption(\"Tabela 01. Característica Salariais Formosa\") %&gt;%\n  add_n() %&gt;%\n  add_difference() %&gt;%\n  bold_p(t = 0.05) %&gt;%\n  bold_labels() %&gt;%\n  italicize_levels()\n\n\n\n\n\nTabela 01. Característica Salariais Formosa\n\n\n\n\n\n\n\n\n\n\n\nVARIAVEIS\nN\nCAPITAL\nN = 451\nINTERIOR\nN = 351\nDifference2\n95% CI2\np-value2\n\n\n\n\nCARGO\n80\n\n\n\n\n\n\n\n\n\n\n\n\n    AUXILIAR\n\n\n17 (38%)\n20 (57%)\n\n\n\n\n\n\n\n\n    DIRETOR\n\n\n12 (27%)\n0 (0%)\n\n\n\n\n\n\n\n\n    GERENTE\n\n\n15 (33%)\n15 (43%)\n\n\n\n\n\n\n\n\n    PRESIDENTE\n\n\n1 (2.2%)\n0 (0%)\n\n\n\n\n\n\n\n\nIDADE\n80\n53 (9)\n46 (9)\n6.7\n2.7, 11\n0.001\n\n\nTEMPOCASA\n80\n13 (7)\n8 (6)\n5.7\n2.7, 8.7\n&lt;0.001\n\n\nEDUCAÇÃO\n80\n\n\n\n\n\n\n\n\n\n\n\n\n    SECUNDÁRIO\n\n\n7 (16%)\n8 (23%)\n\n\n\n\n\n\n\n\n    SUPERIOR\n\n\n38 (84%)\n27 (77%)\n\n\n\n\n\n\n\n\nSALARIO\n80\n6,096 (1,300)\n5,369 (718)\n727\n271, 1,183\n0.002\n\n\n\n1 n (%); Mean (SD)\n\n\n2 Welch Two Sample t-test\n\n\nAbbreviation: CI = Confidence Interval",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Análise Exploratória de Dados (EDA)</span>"
    ]
  },
  {
    "objectID": "exploratoria.html#variáveis-quantitativas",
    "href": "exploratoria.html#variáveis-quantitativas",
    "title": "1  Análise Exploratória de Dados (EDA)",
    "section": "1.5 Variáveis Quantitativas",
    "text": "1.5 Variáveis Quantitativas\n\n1.5.1 Medidas Resumo Geral\nA função mais famosa para a estatística descritiva no R, é a chamada de summary(), que dá a amplitude dos dados.\nA função summary() do pacote basic, retorna boa parte da estatística descritiva como os quartis, média, mediana, mínimo, máximo e as espécies que há na tabela. É importante saber que de ante-mão, quanto mais próxima a mediana estiver da média, maior a probablidade de a destribuição dos dados ser gaussiana. Esta forma de descrição é mais utilizada para uma obtenção rápida dos parâmetros dos dados.\n\nsummary(mercado2$IDADE)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  30.00   43.50   49.00   49.62   55.50   72.00 \n\nsummary(mercado2$TEMPOCASA)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0     3.5    12.0    10.8    16.5    25.0 \n\nsummary(mercado2$SALARIO)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   4187    4894    5660    5693    6270    7481 \n\n\n\n\n1.5.2 Interpretação p/ Salário\n\no 1º quartil(1 st Qu.) indica que 25% dos funcionários têm renda salarial menor ou igual a R$ 4.894 e o 3º quartil(3 rd Qu.) indica que 75% têm renda menor ou igual a R$ 6.306.\nEstes dois valores indicam que 50% dos funcionários tem renda nesse intervalo, o que já nos dá uma idéia de variabilidade da Renda.\nQuanto maior a diferença entre o 3º e o 1º quartil, maior a dispersão da variável.\n\n\n\n\n\n\n\n\n1.5.3 Análise Geral\nO pacote skimr é um função que nos fornece medidas resumo de variáveis de uma base de dados de interesse. Ele pode ser visto como uma alternativa mais completa para a função summary() do R Base para gerar uma tabela geral sobre a base de dados, fornecendo um primeiro olhar sobre o fenômeno estudado.\n\nlibrary(skimr)\n\nmercado |&gt; skim()\n\n\nData summary\n\n\nName\nmercado\n\n\nNumber of rows\n80\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nEDUCAÇÃO\n0\n1\n8\n10\n0\n2\n0\n\n\nCARGO\n0\n1\n7\n10\n0\n4\n0\n\n\nLOCAL\n0\n1\n7\n8\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nIDADE\n0\n1\n49.75\n9.59\n30.0\n43.75\n49.00\n56.0\n72.0\n▃▇▇▃▃\n\n\nTEMPOCASA\n0\n1\n10.89\n7.40\n0.0\n3.75\n12.50\n17.0\n25.0\n▇▃▅▆▃\n\n\nSALARIO\n0\n1\n5777.62\n1138.19\n4186.6\n4894.30\n5673.45\n6306.3\n12465.8\n▇▇▁▁▁\n\n\n\n\n\nVeja que a saída da função skim mostra uma visão geral da base de dados, nos dando informações como número de linhas, número de colunas, e os tipos das colunas\nEla fornece também informações individuais sobre cada coluna da base, separando as colunas por tipo: cada tipo nos dá um conjunto diferente de estatísticas, que façam sentido para aquele tipo de dado.\n\n\n\n\n\n\n\n1.5.4 Relação Gráfica: Variáveis Quantitativas\n\n1.5.4.1 Histograma com Boxplot\nConhecer como as variáveis se relacionam também é um passo muito importante antes da elaboração de um modelo quantitativo.\n\nlibrary(magrittr)\nlibrary(dplyr)\n\nnf = layout(mat = matrix(c(1,2),2,1, byrow = TRUE), height = c(1,2))\n\npar(mar = c(4.1, 3.1, 1.1, 2.1))\n\nboxplot(mercado$SALARIO,\n        col = \"Red\",\n        border = \"Black\",\n        horizontal = TRUE,\n        notch = TRUE)\nhist(mercado$SALARIO, \n     col = \"blue\",\n     freq = TRUE,\n     main = \"Histograma com BoxPlot\",\n     xlab = \"R$ Salário\",\n     ylab = \"Frequência\")\n\n\n\n\n\n\n\n\n\nlibrary(magrittr)\nlibrary(dplyr)\n\nnf = layout(mat = matrix(c(1,2),2,1, byrow = TRUE),height = c(1,2))\n\npar(mar = c(4.1, 3.1, 1.1, 2.1))\n\nboxplot(mercado$IDADE,\n        col = \"Red\",\n        border = \"Black\",\n        horizontal = TRUE,\n        notch = TRUE)\nhist(mercado$IDADE, \n     col = \"blue\",\n     freq = TRUE,\n     main = \"Histograma com BoxPlot\",\n     xlab = \"Idade (anos)\",\n     ylab = \"Frequência\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.5.4.2 Diagrama de Ramos-e-folhas\nVamos investigar como está o comportamento das variáveis através de uma ferramenta chama diagrama de ramos e folhas. Para isso vamos usar o comando abaixo:\n\nstem(mercado2$IDADE)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  3 | 0234\n  3 | 67778899\n  4 | 0000233344\n  4 | 5555556666677778899\n  5 | 00000111222234\n  5 | 5555667889\n  6 | 000234\n  6 | 555579\n  7 | 02\n\nstem(mercado2$SALARIO)\n\n\n  The decimal point is 3 digit(s) to the right of the |\n\n  4 | 234\n  4 | 5666777778889999999\n  5 | 1111111333334\n  5 | 5566777899999\n  6 | 01111122223344\n  6 | 66666667899\n  7 | 133\n  7 | 555\n\n\n\n\n\n\n\n\n\n1.5.4.3 Diagrama de Dispersão\nA função pairs.panels() do pacote psych no R gera um figura com os gráficos de dispersão 2 a 2, os histogramas de cada variável e as correlações das variaveis 2 a 2.\n\nlibrary(psych)\npairs.panels(mercado,\n             method = \"pearson\",\n             density = TRUE,  \n             ellipses = TRUE,\n             smoother = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Análise Exploratória de Dados (EDA)</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Regressão Linear",
    "section": "",
    "text": "2.1 Regressão Simples\nA Técnica de Regressão Linear é uma das mais conhecida e utilizadas na Estatística. È a porta de entrada para diversos modelos preditivos mais sofisticados, já que muitos destes usam conceitos também utilizados na regressão linear. Essencialmente, a regressão linear pode ser utilizada para prever o valor de uma variável quantitativa (dependente) em função das outras variáveis (independentes ou preditoras).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regressão Linear</span>"
    ]
  },
  {
    "objectID": "intro.html#data-set",
    "href": "intro.html#data-set",
    "title": "2  Regressão Linear",
    "section": "2.2 Data Set",
    "text": "2.2 Data Set\nPara ilustrar a regressão simples, vamos começar com um exemplo em que queremos estudar a relação entre idade (variável preditora \\(X_{1}\\)) e Salário (variável dependente Y) com uma amostra de 80 funcionários do Supermercadp Formosa.\n\nLocal: Supermercado Formosa\nAmostra: 80 pessoas\nID : Indetidade do Funcionário\nEDUCAÇÃO : Nível Educacional do Funcionário\nCARGO : Cargo do Funcionário\nLOCAL : Local onde Atua o Funcionário\nIDADE : Idade em anos Completos do Funcionário\nTEMPOCASA : Tempo de Casa\nSALARIO : Salário Mensal do Funcionário em R$\n\nPara ilustrar a regressão simples, vamos começar com um exemplo em que queremos estudar a relação entre idade (variável preditora \\(X_{1}\\)) e Salário (variável dependente Y) com uma amostra de 80 funcionários do Supermercadp Formosa.\nVamos assumir que o salário varia linearmente conforme a idade. Matematicamente, diremos que o salário é uma função linear da idade: salário = \\(Salário \\ = \\ \\beta_{0}+\\beta_{1}*Idade\\). Entretanto, sabemos que esta relação não é determinística, isto é, não necessariamente a diferença salarial entre uma pessoa com 30 anos e outra com 31 será \\(\\beta_{1}\\). Isso ocorre porque há outros fatores que interferem no salário e não estão incluídos no modelo. Este ruído será representado por um termo de erro do modelo:\n\\[ Salário = \\beta_{0} + \\beta_{1} * Idade + erro\\]\nNO modelo de regressão simples tradicional, o termo de erro tem valor esperado igual a7 zero, e isso implica no salário médio das pessoas com determinada idade, denotado por E(Salário), dado pela parte determinística da equação:\n\\[ E(Salário) = \\beta_{0} + \\beta_{1} * Idade\\]\n\\(\\beta_{0} + \\beta_{1}\\) são parâmetros do modelo e podem ser estimados a partir dos dados da amostra. NO exemplo , usaremos os dados amostrais para estimar esses parâmetros. o Primeiro passo e construir um gráfico de dispersão em que colocamos a idade no eixo x e o salário no eixo y.\n\n\n\n\n\n\n2.2.1 Gráfico de Dispersão Geral\nO script utilizado para gerar o gráfico de dispersão no R é mostrado a seguir:\n\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(readxl)\n\nmercado &lt;- read_excel('mercado.xlsx')\n\nPlot2 &lt;- ggplot(mercado, aes(x=IDADE, y= SALARIO))+\n  geom_point(size = 2.5, \n             pch = 21, \n             col = 'black',\n             fill = 'red')+\n  geom_smooth(method=\"lm\", \n              se= TRUE)+\n  theme_bw()+\n  labs(x=\"IDADE\", \n       y=\"SALÁRIO\", \n       title=\"Diagrama de Dispersão Geral\", \n       subtitle = \"Renda Salarial\")\nggplotly(Plot2)\n\n\n\n\n\nO gráfico mostra originalmente um ponto muito distante dos demais, no qual é o salário de um dos diretores da Empresa que ganha R$ 12.465,80\n\nlibrary(readxl)\n\nmercado2 &lt;- read_excel('mercado2.xlsx')\n\nO script utilizado para gerar o gráfico de dispersão no R sem a observação 69 correspondente a (60 anos; R$ 12.465,80) é mostrado a seguir:\n\n\n\n\n\n\n\n2.2.2 Gráfico de Dispersão Sem obs: 69º\n\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(readxl)\n\nmercado2 &lt;- read_excel('mercado2.xlsx')\n\nPlot3 &lt;- ggplot(mercado2, aes(x=IDADE, y= SALARIO))+\n  geom_point(size = 2.5, \n             pch = 21, \n             col = 'black',\n             fill = 'red')+\n  geom_smooth(method=\"lm\", \n              se= TRUE)+\n  theme_bw()+\n  labs(x=\"IDADE\", \n       y=\"SALÁRIO\", \n       title=\"Diagrama de Dispersão sem Outliers\", \n       subtitle = \"Renda Salarial\")\nggplotly(Plot3)\n\n\n\n\n\nO gráfico mostra que há uma tendência de crescimento do salário quando a idade aumenta, ilustrado pela reta inclinada, que chamaremos de reta de mínimos quadrados.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regressão Linear</span>"
    ]
  },
  {
    "objectID": "intro.html#reta-de-mínimos-quadrados",
    "href": "intro.html#reta-de-mínimos-quadrados",
    "title": "2  Regressão Linear",
    "section": "2.3 Reta de Mínimos Quadrados",
    "text": "2.3 Reta de Mínimos Quadrados\nA seguir, vamos ver como encontrar a reta que estabelece uma relação entre as duas variáveis:\n\\[ \\hat{y} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}x}\\]\nO símbolo “^” em \\(\\beta_{0}\\) e \\(\\beta_{1}\\) indica que estamos estimando os parâmetros do modelo populacional , já que contaremos apenas com dados amostrais no nosso cálculo. \\(\\hat{y}\\) é o valo previsto do salário médio dos funcionários com idade “x”.\nO objetivo é obter estimadores \\(\\hat{\\beta_{0}}\\) e \\(\\hat{\\beta_{1}}\\), isto é, a reta, que melhor se ajusta aos dados. Como critério de ajuste utilizaremos a “Soma de Quadrados dos Resíduos” (SQR), definida a seguir. O resíduo da7 observação “i” da amostra é a diferença entre o seu valor observado \\(y_{i}\\) e o valor previsto \\(\\hat{y}_{i}\\).\n\\[ SQR = min \\sum_{i=1}^{n} (y_{i}-\\hat{y}_{i})^{2} = min \\sum_{i=1}^{n} e_{i}^{2}\\]\nem que \\(e_{i}\\) é o resíduo da observação “i”.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regressão Linear</span>"
    ]
  },
  {
    "objectID": "intro.html#coeficiente-de-regressão-linear",
    "href": "intro.html#coeficiente-de-regressão-linear",
    "title": "2  Regressão Linear",
    "section": "2.4 Coeficiente de Regressão Linear",
    "text": "2.4 Coeficiente de Regressão Linear\nPara fazer uma análise de regressão no R, usaremos a função lm, do pacote basic, e os dados do Supermercado Formosa. A sintaxe para rodar a regressão linear simples é lm(y~x).\n\n2.4.1 Tabela de Resultado Padrão\nUma das tarefas mais corriqueiras dos analista de dados é a produção de tabelas. Seja para apresentar frequências, estatísticas descritivas (media, mediana, moda, etc), seja para apresentar resultados de modelos de regressão.\nTabela de resultados referente ao modelo de regressão linear simples com n=80.\n\nModelo1 = lm(mercado$SALARIO~mercado$IDADE)\nsummary(Modelo1)\n\n\nCall:\nlm(formula = mercado$SALARIO ~ mercado$IDADE)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1213.0  -505.3   -65.7   340.9  5872.4 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    1818.11     504.51   3.604  0.00055 ***\nmercado$IDADE    79.59       9.96   7.991  9.8e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 849.4 on 78 degrees of freedom\nMultiple R-squared:  0.4501,    Adjusted R-squared:  0.4431 \nF-statistic: 63.86 on 1 and 78 DF,  p-value: 9.795e-12\n\n\nNa saída acima podemos ver os estimadores \\(\\hat{\\beta_{0}}\\) e \\(\\hat{\\beta_{1}}\\) (estimate), seus erros padrão (Std. Error), a estatística t (t value) e o valor-p do teste de hipótese (Pr(&gt;|t|)).\nOs etimadores \\(\\hat{\\beta_{0}}\\) e \\(\\hat{\\beta_{1}}\\) possuem um erro padrão que depende de vários fatores, entre eles o tamanho da amostra e o desvio-padrão do erro do modelo. Com esses valores podemos construir uma estimativa intervalar, com determinado nível de confiança, para os parâmetros populacionais desconhecidos \\(\\beta_{0}\\) e \\(\\beta_{1}\\).\n\nModelo2 = lm(mercado2$SALARIO~mercado2$IDADE)\nsummary(Modelo2)\n\n\nCall:\nlm(formula = mercado2$SALARIO ~ mercado2$IDADE)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1177.94  -445.50   -14.98   417.77  1263.66 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    2165.813    310.449   6.976 9.21e-10 ***\nmercado2$IDADE   71.083      6.144  11.569  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 520.2 on 77 degrees of freedom\nMultiple R-squared:  0.6348,    Adjusted R-squared:   0.63 \nF-statistic: 133.8 on 1 and 77 DF,  p-value: &lt; 2.2e-16\n\n\nA presença desse outlier tem consequências importantes para o resultado da regressão. Os valores de \\(\\hat{\\beta}_{0}\\) e \\(\\hat{\\beta}_{1}\\) mudam de (1818,11; 79,59) para (2165,81; 71,08), respectivamente, o que faz com que os valores previstos também mudem, especialmente nos extremos, isto é, para idades muito baixas e muito altas. Isso significa que esse ponto, além de outlier, é um ponto influente, isto é, a presença dele muda as estimativas do modelo.\n\n\n\n\n\n\n\n2.4.2 Tabela de Resultados Personalizada\nO pacote gtsummary fornece uma maneira simples e sofisticada de criar tabelas para apresentar os resultados de modelos de regressão no R. A função tbl_regression() pega um objeto de modelo de regressão em R e retorna uma tabela formatada de resultados do modelo de regressão que está pronta para publicação.\nTabela de resultados referentes ao modelo de regressão liear simples sem a observação 69.\n\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(gtsummary)\n\nModelo2 = lm(mercado2$SALARIO~mercado2$IDADE) \n\nModelo2 %&gt;%\ntbl_regression() %&gt;%\n  add_global_p() %&gt;%\n  bold_p(t = 0.05) %&gt;%\n  bold_labels() %&gt;%\n  italicize_levels() %&gt;% \n  modify_header(label = \"**VARIAVEIS**\") %&gt;%\n  modify_caption(\"Tabela 01. Modelo de Regressao Simples do Salário em função da Idade, Formosa - 2023.\")\n\n\n\n\n\nTabela 01. Modelo de Regressao Simples do Salário em função da Idade, Formosa - 2023.\n\n\nVARIAVEIS\nBeta\n95% CI\np-value\n\n\n\n\nmercado2$IDADE\n71\n59, 83\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regressão Linear</span>"
    ]
  },
  {
    "objectID": "intro.html#análise-de-resíduo",
    "href": "intro.html#análise-de-resíduo",
    "title": "2  Regressão Linear",
    "section": "2.5 Análise de Resíduo",
    "text": "2.5 Análise de Resíduo\nOs Testes de hipóteses anteriores só têm validade se as suposições do modelo estiverem satisfeitas. As suposições são as mesmas do modelo de regressão simples ou multipla: o erro deve ter distribuiçao normal com média 0, variância constante e independentes.\nExistem diversos pacotes na linguagem R para fazer a análise dos resíduos, mas será enfatizado apenas os gráficos mai comuns, que podem ser feitos sem a instalação de nehhum pacote adicional.\nO script a seguir cria os gráficos para regressão linear simples com n=80.\n\npar(mfrow=c(2,2))\nplot(Modelo1)\n\n\n\n\n\n\n\n\nO primeiro gráfico é chamado de (Residuals vs Fitted) mostra que a observação 69 tem um resíduo extremamente alto, considerando um outlier da regressão.\nO segundo gráfico, na direita superior, é chamado de (QQ plot), e é uma alternativa ao histograma para averiguar se há normalidade dos erros. Espera-se que, se a distribuição for normal, os pontos estarão próximos a uma reta. A observação 69 está bem longe dessa reta, colocando em dúvida a suposição de que os erros têm distribuição normal.\nO gráfico da esquerda inferior (Scale-Location) pode ser utilizado para averiguar se a variância é constante. Quando a variância é constante, a linha cinza-claro não apresenta oscilações significativas ao longo do eixo x.\nO gráfico inferior a direita (Residuals vs Leverage) nos ajuda a identificar pontos influentes na regressão. Utiliza-se como critério a distância de Cook.\nA distância de Cook mede o quanto determinada observação influência o resultado da regressão.\nPontos acima da linha tracejada inferior são considerados influentes, caso da observação 69.\nO script a seguir cria os gráficos para regressão linear simples com n=79, ou seja, sem a observação 69, considerada um outlier.\n\npar(mfrow=c(2,2))\nplot(Modelo2)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regressão Linear</span>"
    ]
  },
  {
    "objectID": "intro.html#normalidade-dos-resíduos",
    "href": "intro.html#normalidade-dos-resíduos",
    "title": "2  Regressão Linear",
    "section": "2.6 Normalidade dos Resíduos",
    "text": "2.6 Normalidade dos Resíduos\n\nshapiro.test(Modelo2$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  Modelo2$residuals\nW = 0.97771, p-value = 0.1819",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regressão Linear</span>"
    ]
  },
  {
    "objectID": "intro.html#independência-dos-resíduos",
    "href": "intro.html#independência-dos-resíduos",
    "title": "2  Regressão Linear",
    "section": "2.7 Independência dos Resíduos",
    "text": "2.7 Independência dos Resíduos\n\nlibrary(car)  \n\ndurbinWatsonTest(Modelo2)\n\n lag Autocorrelation D-W Statistic p-value\n   1       0.1755387      1.612743   0.072\n Alternative hypothesis: rho != 0",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regressão Linear</span>"
    ]
  },
  {
    "objectID": "intro.html#homocedasticidade-dos-resíduos-breusch-pagan",
    "href": "intro.html#homocedasticidade-dos-resíduos-breusch-pagan",
    "title": "2  Regressão Linear",
    "section": "2.8 Homocedasticidade dos Resíduos (Breusch-Pagan)",
    "text": "2.8 Homocedasticidade dos Resíduos (Breusch-Pagan)\n\nlibrary(lmtest)  \n\nbptest(Modelo2)\n\n\n    studentized Breusch-Pagan test\n\ndata:  Modelo2\nBP = 0.092001, df = 1, p-value = 0.7616",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regressão Linear</span>"
    ]
  },
  {
    "objectID": "intro.html#previsão-simples",
    "href": "intro.html#previsão-simples",
    "title": "2  Regressão Linear",
    "section": "2.9 Previsão Simples",
    "text": "2.9 Previsão Simples\nPara fazer a previsão do salário usando a linguagem R, pode-se utilizar a função predict. Por exemplo, que estamos interessados prever o salário de um funcionário de 40 anos e outro de 50 anos.\nO script no R é dado a seguir:\n\nPrevisao1 = data.frame(IDADE = c(40,50))\n\npredict(Modelo1, \n        newdata = Previsao1, \n        interval = \"prediction\")\n\n        fit      lwr      upr\n1  4205.757 2459.748 5951.766\n2  4364.933 2627.389 6102.477\n3  4444.521 2710.885 6178.158\n4  4524.109 2794.162 6254.057\n5  4683.286 2960.056 6406.516\n6  4762.874 3042.670 6483.078\n7  4762.874 3042.670 6483.078\n8  4762.874 3042.670 6483.078\n9  4842.462 3125.060 6559.864\n10 4842.462 3125.060 6559.864\n11 4922.050 3207.225 6636.875\n12 4922.050 3207.225 6636.875\n13 5001.638 3289.165 6714.111\n14 5001.638 3289.165 6714.111\n15 5001.638 3289.165 6714.111\n16 5001.638 3289.165 6714.111\n17 5160.815 3452.364 6869.265\n18 5240.403 3533.621 6947.184\n19 5240.403 3533.621 6947.184\n20 5240.403 3533.621 6947.184\n21 5319.991 3614.650 7025.332\n22 5319.991 3614.650 7025.332\n23 5399.579 3695.449 7103.709\n24 5399.579 3695.449 7103.709\n25 5399.579 3695.449 7103.709\n26 5399.579 3695.449 7103.709\n27 5399.579 3695.449 7103.709\n28 5399.579 3695.449 7103.709\n29 5479.167 3776.017 7182.317\n30 5479.167 3776.017 7182.317\n31 5479.167 3776.017 7182.317\n32 5479.167 3776.017 7182.317\n33 5479.167 3776.017 7182.317\n34 5558.755 3856.356 7261.154\n35 5558.755 3856.356 7261.154\n36 5558.755 3856.356 7261.154\n37 5558.755 3856.356 7261.154\n38 5638.343 3936.464 7340.223\n39 5638.343 3936.464 7340.223\n40 5717.931 4016.341 7419.522\n41 5717.931 4016.341 7419.522\n42 5797.520 4095.987 7499.052\n43 5797.520 4095.987 7499.052\n44 5797.520 4095.987 7499.052\n45 5797.520 4095.987 7499.052\n46 5797.520 4095.987 7499.052\n47 5877.108 4175.401 7578.814\n48 5877.108 4175.401 7578.814\n49 5877.108 4175.401 7578.814\n50 5956.696 4254.585 7658.806\n51 5956.696 4254.585 7658.806\n52 5956.696 4254.585 7658.806\n53 5956.696 4254.585 7658.806\n54 6036.284 4333.538 7739.029\n55 6115.872 4412.261 7819.483\n56 6195.460 4490.753 7900.167\n57 6195.460 4490.753 7900.167\n58 6195.460 4490.753 7900.167\n59 6195.460 4490.753 7900.167\n60 6275.048 4569.016 7981.081\n61 6275.048 4569.016 7981.081\n62 6354.636 4647.049 8062.224\n63 6434.225 4724.854 8143.595\n64 6434.225 4724.854 8143.595\n65 6513.813 4802.430 8225.195\n66 6593.401 4879.780 8307.021\n67 6593.401 4879.780 8307.021\n68 6593.401 4879.780 8307.021\n69 6593.401 4879.780 8307.021\n70 6752.577 5033.802 8471.352\n71 6832.165 5110.476 8553.854\n72 6911.753 5186.927 8636.580\n73 6991.341 5263.156 8719.527\n74 6991.341 5263.156 8719.527\n75 6991.341 5263.156 8719.527\n76 6991.341 5263.156 8719.527\n77 7150.518 5414.955 8886.081\n78 7309.694 5565.882 9053.506\n79 7389.282 5641.023 9137.541\n80 7548.458 5790.668 9306.249\n\n\nPode-se concluir, que um funcionário de 40 anos terá um salário entre R$  e R$  com 95% de probabilidade.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regressão Linear</span>"
    ]
  },
  {
    "objectID": "intro.html#regressão-múltipla",
    "href": "intro.html#regressão-múltipla",
    "title": "2  Regressão Linear",
    "section": "2.10 Regressão Múltipla",
    "text": "2.10 Regressão Múltipla\nGeralmente os modelos de regressão linear simples possuem alto erro padrão e baixo \\(R^{2}\\). Isso porque a variável Y(salário) é explicada por diversos fatores, não só pela idade do funcionário. Portanto, seria interessante utilizar mais de uma variável preditora no modelo para prever o salário com maior precisão.\nQuando há mais de uma variável preditora, temos o Modelo de Reressão Múltipla.\nEm termos gerais, um modelo de regressão linear múltipla é dado por:\n\\[ Y_{i} = \\beta_{0} + \\sum_{j=1}^{k} \\beta_{j} X_{ij}+\\epsilon_{i}\\]\nCom adição de variáveis ao modelo, espera-se que o \\(R^{2}\\) aumente consideravelmente. Na verdade, mesmo uma variável irrelevante para o modelo provocará um aumento (insignificante do \\(R^{2}\\)). Desta forma, não é recomendável utilizar o \\(R^{2}\\) para comparar um modelo com um variável preditora e outro modelo com duas preditoras. O modelo com duas preditoras sempre possuirá um \\(R^{2}\\) maior, principalmente se o número de variáveis prediotas for grande em comparação com o tamanho da amostra.\nO \\(R^{2}_{ajustado}\\) é uma medida que permite comparar modelos com diferentes tamanhos, pois para cada variável adicionada ao modelo a medida sofre uma penalização.\nO \\(R^{2}_{ajustado}\\) de um modelo com mais variáveis só aumentará se essa nova variável ajudar a prever o Y. Caso contrário, o \\(R^{2}_{ajustado}\\) pode diminuir em relação ao modelo sem tal variável.\nO modelo de regressão linear múltipla utilizará as variáveis: idade, tempo de casa, educação, cargo e local de trabalho para prever o salário.\nA função utilizada para rodar a regressão múltipla é a mesma que usamos para o modelo1 de regressão simples, mas agora colocaremos as variáveis preditoras após o “~” e separadas por “+”.\n\nModelo3 = lm(mercado2$SALARIO~mercado2$EDUCAÇÃO+mercado2$CARGO+mercado2$LOCAL+mercado2$IDADE+mercado2$TEMPOCASA)\nsummary(Modelo3)\n\n\nCall:\nlm(formula = mercado2$SALARIO ~ mercado2$EDUCAÇÃO + mercado2$CARGO + \n    mercado2$LOCAL + mercado2$IDADE + mercado2$TEMPOCASA)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-786.95 -246.45  -14.05  188.33 1315.44 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               3547.185    300.270  11.813  &lt; 2e-16 ***\nmercado2$EDUCAÇÃOSUPERIOR  128.199    108.170   1.185 0.239849    \nmercado2$CARGODIRETOR      737.071    143.260   5.145 2.22e-06 ***\nmercado2$CARGOGERENTE      345.073     93.222   3.702 0.000416 ***\nmercado2$LOCALINTERIOR     139.279     94.337   1.476 0.144198    \nmercado2$IDADE              18.689      7.306   2.558 0.012636 *  \nmercado2$TEMPOCASA          75.007      9.198   8.154 7.89e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 365.8 on 72 degrees of freedom\nMultiple R-squared:  0.8311,    Adjusted R-squared:  0.8171 \nF-statistic: 59.07 on 6 and 72 DF,  p-value: &lt; 2.2e-16\n\n\nNote que a saída do R para regressão múltipla é bem similar à da simples, A diferença é que agora há várias linhas, uma para cada variável independente, com suas estimativas, erros padrão, estatística t e valor-p.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regressão Linear</span>"
    ]
  },
  {
    "objectID": "intro.html#interpretação-dos-coeficientes",
    "href": "intro.html#interpretação-dos-coeficientes",
    "title": "2  Regressão Linear",
    "section": "2.11 Interpretação dos Coeficientes",
    "text": "2.11 Interpretação dos Coeficientes\nO primeiro coeficiente, relativo a variável Educação, motra uma estimativa da diferença média entre os salários dos funcionários com nível superior e com nível secundário, mantida todas as variáveis constante. O valor estimado é 128,199. Entretanto, o erro padrão dessa estimativa é grande (108,17) e acarreta um valor-p alto = 0.23. Portanto, não conseguimos rejeitar a hipótese de que o coeficiente \\(\\beta_{1}\\) é diferente de zero. Desta forma, variável “Educação” não se mostrou significante no modelo, e será retirada.\nO segundo o terceiro coeficiente estão relacionados ao Cargo. O valor-p de ambas é muito baixo, menor que 0.0001, indicando fortes evidências estatísticas de que esses coeficientes na população são diferentes de zero.\nO quarto coeficiente, relativo a Local de Trabalho, assim como o primeiro, não é significante (valor-p = 0.14). Portanto, não há evidências estatísticas ao nível de 95% de significância de que os salários médios na capital e no interior são diferentes.\nO quinto coeficiente, relativo a Idade, mostra que o salário médio tem um aumento estimado de 18.68 por ano. Ao nível de significância de 0.05, podemos dizer que há evidências estatísticas de que o coeficiente na população é diferente de zero.\nO sexto coeficiente, relativo ao Tempo de Casa, indica que a cada ano dicional do funcionário no supermercado há um amento estimado no salário de 75,007 reais. Neste caso, conclui-se também que há evidências de que o coeficiente relativo ao tempo de casa na população \\(\\beta_{6}\\), é diferente de zero, pois valor-p &lt; 0.0001.\n\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(gtsummary)\n\nModelo3 = lm(mercado2$SALARIO~mercado2$EDUCAÇÃO+mercado2$CARGO+mercado2$LOCAL+mercado2$IDADE+mercado2$TEMPOCASA)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regressão Linear</span>"
    ]
  },
  {
    "objectID": "intro.html#modelo-final",
    "href": "intro.html#modelo-final",
    "title": "2  Regressão Linear",
    "section": "2.12 Modelo Final",
    "text": "2.12 Modelo Final\nCom base no modelo anterior, verificou-se que, as variáveis: Nível Educacional e Local onde Atua o Funcionário não se mostraram estatisticamente significantes, com isso foram retirada na composição do modelo final.\nNote que o modelo final não fica guardado em nenhum objeto no R. È preciso rodar novamente apenas as variáveis selecionadas.\n\nModelo4 = lm(mercado2$SALARIO~+mercado2$CARGO+mercado2$IDADE+mercado2$TEMPOCASA)\nsummary(Modelo4)\n\n\nCall:\nlm(formula = mercado2$SALARIO ~ +mercado2$CARGO + mercado2$IDADE + \n    mercado2$TEMPOCASA)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-807.51 -215.10  -37.22  180.78 1274.19 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           3696.434    292.911  12.620  &lt; 2e-16 ***\nmercado2$CARGODIRETOR  673.404    137.882   4.884 5.84e-06 ***\nmercado2$CARGOGERENTE  326.859     93.630   3.491 0.000815 ***\nmercado2$IDADE          19.932      7.347   2.713 0.008292 ** \nmercado2$TEMPOCASA      72.340      9.001   8.037 1.10e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 369.7 on 74 degrees of freedom\nMultiple R-squared:  0.8227,    Adjusted R-squared:  0.8131 \nF-statistic: 85.84 on 4 and 74 DF,  p-value: &lt; 2.2e-16\n\n\nPode-se viasualizar os resultados do modelo final de forma gráfica.\n\nlibrary(GGally)\n\nggcoef(Modelo4,\n       exclude_intercept = FALSE,\n       vline_color = \"red\",\n       vline_linetype = \"solid\",\n       errorbar_color = \"blue\",\n       errorbar_height = 0.5,\n       size = 5, \n       shape = 18)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regressão Linear</span>"
    ]
  },
  {
    "objectID": "intro.html#comparação-dos-modelos",
    "href": "intro.html#comparação-dos-modelos",
    "title": "2  Regressão Linear",
    "section": "2.13 Comparação dos Modelos",
    "text": "2.13 Comparação dos Modelos\n\nlibrary(car)\n\nAIC(Modelo1,Modelo2, Modelo3, Modelo4)\n\n        df      AIC\nModelo1  3 1310.126\nModelo2  3 1216.318\nModelo3  8 1165.377\nModelo4  6 1165.234\n\nBIC(Modelo1,Modelo2, Modelo3, Modelo4)\n\n        df      BIC\nModelo1  3 1317.272\nModelo2  3 1223.426\nModelo3  8 1184.333\nModelo4  6 1179.451",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regressão Linear</span>"
    ]
  },
  {
    "objectID": "intro.html#multicolinearidade",
    "href": "intro.html#multicolinearidade",
    "title": "2  Regressão Linear",
    "section": "2.14 Multicolinearidade",
    "text": "2.14 Multicolinearidade\nQuando uma variável preditora possui alta correlação com outras variáveis preditoras ou com uma combinação delas, temos um problema de multicolinearidade na regressão.\nQuando isso ocorre, as estimativas dos coeficientes apresentam alto erro padrão e geralmente são não significantes, tornando difícil avaliar o efeito de cada preditor no modelo.\nHá diversas formas de detectar multicolinearidade, sendo as mais utilizadas a Tolerância e o VIF(Variance inflation fator). Ambas baseiam-se em quanto uma variável preditora pode ser explicada pela combinação linear das outras variáveis preditoras.\nUma boa medida disto é o \\(R^{2}\\) da regressão em que a variável preditora “j” é explicada por todas as outras variáveis preditora. Um \\(R^{2}\\) alto indica multicolinearidade.\nA Tolerância e o VIF da variável preditora “j” são dados por:\n\\[ Tolerância = 1- R^{2}_{j}\\]\n\\[ VIF =  \\frac{1}{Tolerância} = \\frac{1}{1- R^{2}_{j}}\\]\nO cálculo do VIF na linguagem R pode ser feito utilizando o pacote car.\n\nlibrary(car)\nvif(Modelo3)\n\n                       GVIF Df GVIF^(1/(2*Df))\nmercado2$EDUCAÇÃO  1.062805  1        1.030925\nmercado2$CARGO     1.502308  2        1.107107\nmercado2$LOCAL     1.296742  1        1.138746\nmercado2$IDADE     2.859579  1        1.691029\nmercado2$TEMPOCASA 2.704690  1        1.644594\n\n\nVinhos que nenhuma variável possui VIF (ou GVIF) maior que 5, portanto, não temos problema de multicolinearidade no nosso modelo.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regressão Linear</span>"
    ]
  },
  {
    "objectID": "intro.html#previsão-multipla",
    "href": "intro.html#previsão-multipla",
    "title": "2  Regressão Linear",
    "section": "2.15 Previsão Multipla",
    "text": "2.15 Previsão Multipla\nCom todas as suposições do modelo satisfeita e com o modelo final selecionado, o próximo passo é fazer Previsões.\nDeve-se primeiro montar um dataframe com os valores das variáveis preditoras que utilizam para estimar os salários.\nNovamente usamos a função predict para prever um Salário de uma pessoa com 30 anos, 5 anos de casa e no Cargo de Diretor:\n\nPrevisao2 = data.frame(CARGO=\"DIRETOR\",\n                       IDADE=30,\n                       TEMPODECASA=50)\n\npredict(Modelo4, \n        newdata = Previsao2, \n        interval = \"prediction\")\n\n        fit      lwr      upr\n1  4693.592 3923.303 5463.881\n2  4623.616 3859.252 5387.980\n3  4498.868 3737.852 5259.884\n4  4917.999 4155.997 5680.002\n5  4813.184 4056.018 5570.349\n6  4506.256 3749.320 5263.192\n7  4433.917 3675.297 5192.536\n8  4867.955 4113.075 5622.836\n9  5394.265 4627.929 6160.602\n10 5359.426 4598.685 6120.167\n11 4835.479 4083.573 5587.385\n12 4800.640 4045.135 5556.144\n13 5037.591 4285.119 5790.063\n14 5217.110 4463.445 5970.775\n15 5289.450 4533.988 6044.912\n16 4820.572 4065.176 5575.967\n17 5222.134 4471.434 5972.835\n18 5025.047 4272.200 5777.894\n19 4698.188 3942.045 5454.330\n20 5493.925 4740.598 6247.252\n21 5007.479 4258.306 5756.651\n22 4972.639 4217.274 5728.005\n23 4810.391 4054.723 5566.060\n24 4810.391 4054.723 5566.060\n25 5064.911 4310.757 5819.065\n26 4738.052 3979.490 5496.614\n27 5678.469 4924.396 6432.541\n28 6005.328 5241.877 6768.778\n29 5012.503 4254.755 5770.252\n30 5336.702 4589.889 6083.515\n31 5374.202 4624.816 6123.588\n32 5301.862 4551.653 6052.071\n33 5770.740 5016.005 6525.475\n34 5104.775 4348.183 5861.367\n35 5177.115 4422.874 5931.355\n36 6007.691 5245.762 6769.621\n37 5523.660 4740.972 6306.347\n38 4942.527 4185.117 5699.937\n39 5738.264 4988.504 6488.024\n40 5107.139 4353.490 5860.787\n41 5758.196 5009.307 6507.086\n42 5127.070 4371.678 5882.463\n43 5309.250 4552.699 6065.801\n44 5271.750 4520.919 6022.581\n45 5453.930 4701.506 6206.353\n46 6249.667 5489.486 7009.848\n47 5147.002 4389.587 5904.418\n48 5870.400 5121.297 6619.503\n49 6124.919 5370.825 6879.013\n50 6107.351 5353.451 6861.251\n51 5493.794 4738.152 6249.435\n52 5927.832 5177.453 6678.211\n53 6563.736 5788.112 7339.360\n54 6054.943 5303.662 6806.225\n55 6474.074 5712.806 7235.342\n56 6349.327 5593.158 7105.495\n57 6059.968 5307.431 6812.504\n58 6494.006 5733.487 7254.526\n59 6406.513 5638.213 7174.812\n60 6404.098 5645.761 7162.435\n61 6621.117 5853.266 7388.969\n62 6207.011 5454.077 6959.945\n63 6009.924 5256.730 6763.117\n64 6755.668 5987.661 7523.674\n65 6703.260 5936.389 7470.130\n66 6578.512 5810.943 7346.081\n67 6738.345 5973.885 7502.806\n68 6795.531 6028.591 7562.472\n69 6763.056 5995.766 7530.345\n70 7217.026 6446.072 7987.980\n71 6274.195 5509.755 7038.634\n72 6476.306 5706.027 7246.586\n73 6583.486 5818.580 7348.391\n74 6750.512 5978.823 7522.201\n75 7127.364 6351.200 7903.529\n76 6116.971 5335.527 6898.416\n77 7264.278 6492.435 8036.121\n78 7356.550 6583.564 8129.535\n79 6723.009 5937.601 7508.418",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regressão Linear</span>"
    ]
  },
  {
    "objectID": "intro.html#machine-learning-no-r",
    "href": "intro.html#machine-learning-no-r",
    "title": "2  Regressão Linear",
    "section": "2.16 Machine Learning no R",
    "text": "2.16 Machine Learning no R\nPara avaliar a performance de um modelo de previsão ou classificação, quando for aplicado à população, não devemos utilizar a mesma amostra considerada para obtê-lo. Isso tende a fornecer resultados otimistas, camuflando inclusive a eventual ocorrência de overfitting.\nDeve-se avaliar o modelo aplicando-o outra amostra dessa população, independente da amostra empregada para o desenvolvimento.\nAs métricas a serem consideradas para avaliação do modelo serão calculadas a partir dessa outra amostra.\nEm geral, selecionamos uma amostra da população e a dividirmos aleatoriamente em duas partes. Uma parte para desenvolvimento que se denomina Amostra de Treinamento ou aprendizado, e a outra denominada Amostra Teste para os teste do modelo obtido.\n\n2.16.1 Separando a Base em Treino e Teste\nNa maioria das vezes, quando existe um conjunto de dados para utilizar na construção de um modelo, precisa-se fazer uma separação entre o que chamamos de treino e teste. O que costuma-se chamar de base treino, é o conjunto de dados que utilizaremos na construção do modelo.\nPorém, para sabermos se o modelo não tem problemas como overfitting, precisamos testar o que foi rodado com a base treino, utilizando o que costumamos chamar de base teste.\nNo R, fazer essa separação é muito simples. Principalmente quando utilizamos o pacote caret, provavelmente o mais utilizado em modelagem estatística.\n\nlibrary(caret)\nlibrary(recipes)\n\n\nBase_Treino &lt;- createDataPartition(mercado$SALARIO, \n                                   p = 0.8, \n                                   list = FALSE)\nTreino &lt;- mercado[Base_Treino,]\nTeste &lt;- mercado[-Base_Treino,]\n\nModelo_Final &lt;- lm(mercado$SALARIO~+\n                  mercado$CARGO+\n                  mercado$IDADE+\n                  mercado$TEMPOCASA, data = Treino)\n\nsummary(Modelo_Final)\n\n\nCall:\nlm(formula = mercado$SALARIO ~ +mercado$CARGO + mercado$IDADE + \n    mercado$TEMPOCASA, data = Treino)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-807.51 -212.79  -22.25  180.09 1274.19 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             3696.434    292.911  12.620  &lt; 2e-16 ***\nmercado$CARGODIRETOR     673.404    137.882   4.884 5.84e-06 ***\nmercado$CARGOGERENTE     326.859     93.630   3.491 0.000815 ***\nmercado$CARGOPRESIDENTE 6271.333    379.275  16.535  &lt; 2e-16 ***\nmercado$IDADE             19.932      7.347   2.713 0.008292 ** \nmercado$TEMPOCASA         72.340      9.001   8.037 1.10e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 369.7 on 74 degrees of freedom\nMultiple R-squared:  0.9012,    Adjusted R-squared:  0.8945 \nF-statistic:   135 on 5 and 74 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regressão Linear</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Regressão Logística",
    "section": "",
    "text": "A regressão logística permite estimar a probabilidade de que uma observação pertença a cada um \\(k\\) grupos (ou categorias) predeterminados. Posteriormente, podemos classificá-la em um desses grupos, de acordo com os valores dessas probabilidades.\nUm dos problemas operacionais mais complexos na aplicação da regressão logística é a diferenciação correta e clara dos dois grupos considerados, evitando qualquer tipo de ambiguidade.\nPor exemplo, no caso da análise de crédito, a definição do que seja um Bom ou um Mal Cliente é extremamente controversa.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regressão Logística</span>"
    ]
  },
  {
    "objectID": "ridge.html",
    "href": "ridge.html",
    "title": "4  Regressão Ridge",
    "section": "",
    "text": "4.1 Métodos pra Determinar \\(k\\)\nA regressão de Cume, Cumeeira, em Crista ou chamada Ridge Regression, proposta por Hoerl e Kennard (1970a), é um dos vários métodos propostos para remediar os problemas de multicolinearidade, alterando o método de mínimos quadrados para permitir estimadores viesados dos coeficientes de regressão.\nQuando um estimador tem um viés pequeno e é substancialmente mais preciso que o estimador não viesado, este pode ser escolhido desde que tenha grande probabilidade de estar próximo do valor verdadeiro (Hoerl; Kennard 1970b). Assim a probabilidade do estimador de cumeeira estar próximo do valor verdadeiro é muito maior que para o estimador não viesado de mínimos quadrados ordinais (NETER et al., 1996).\nUma medida da combinação do efeito do viés e da variação amostral é o valor esperado do quadrado do desvio do estimador e do valor verdadeiro. Esta medida é chamada de Erro Médio Quadrático (EMQ), e pode ser escrita como,\n\\[ E(\\hat{\\beta}-\\beta)^{2} = V(\\hat{\\beta}) + [E(\\hat{\\beta})-\\beta]^{2}\\]\nDessa maneira, o EMQ é igual à variância do estimador mais o viés ao quadrado. Note que se o estimador for não viesado, o erro médio quadrático é igual ao estimador da variância. Pelo método de mínimos quadrados, o coeficiente \\(\\beta\\) pode ser estimado como,\n\\[ \\hat{\\beta} = (X^{'}X)^{-1}X^{'}Y \\]\ne as estimativas e suas variâncias poderão ser incertas na presença de multicolinearidade. A regressão de cume consiste na adição de coeficientes a diagonal principal da matriz de correlações \\((X^{'}X)^{-1}\\) , causando um decréscimo na variância das estimativas. Desta maneira, o estimador de cume de \\(\\beta\\) é obtido por,\n\\[\\hat{\\beta} = (X^{'}X + k)^{-1}X^{'}Y\\] Sendo \\(k= diagonal(k_{1},k_{2},...,k_{p}), k_{i} \\geq 0\\), onde um procedimento bastante usado é \\(k=kI\\), \\(k_{i} \\geq 0\\) . O estimador em cume é na verdade uma família de estimadores, onde \\(k\\) é um valor pequeno que deve ser escolhido a critério do pesquisador. Em geral, aumenta-se gradativamente o valor de \\(k\\) até que os estimadores dos coeficientes tornam-se estáveis, não variam. Se a escolha for \\(k_{i} = 0\\), para todo \\(i\\), tem-se o estimador de Mínimos Quadrados (NETER; WASSERMAN, 1974), (DRAPER; SMITH, 1981) e (ELIAN, 1998).\nQuando os dados possuem traços de multicolinearidade sempre existe um valor para o parâmetro \\(k\\) no qual os estimadores de Regressão em Crista produzem um Quadrado Médio do Erro (QME) menor do que o QME produzido pelos Estimadores de mínimos quadrados ordinários (HOERL; KENNARD, 1970a), (NETER; WASSERMAN, 1974), (DRAPER; SMITH, 1981), (ELIAN, 1998),\nA função estimada pela Regressão em Crista produz predições com novas observações que tendem a serem mais precisas do que as feitas pela função estimada pelo método de mínimos quadrados ordinários, quando as variáveis independentes são correlacionadas e a nova observação segue o mesmo padrão de multicolinearidade, esta precisão na predição de novas observações é favorecida pela Regressão de Cume, especialmente quando a multicolinearidade é forte (NETER; WASSERMAN, 1974), (DRAPER; SMITH, 1981), (ELIAN, 1998).\nUm valor ideal para o parâmetro \\(K\\), o qual resulta em um menor QME que o obtido pelo Método de Mínimos Quadrados Ordinários (MQO) depende do vetor de parâmetro desconhecido e da variância do erro também desconhecido (HOERL; KENNARD, 1970a). Conseqüentemente, K precisa ser determinado empiricamente ou obtido dos dados, e não é possível determinar o valor ideal do parâmetro de cume K. Muitos métodos têm sido propostos para obter os valores apropriados, mas não existe um consenso de qual método é o mais adequado. Assim, o parâmetro de ridge \\(K\\) será estimado a partir de dois métodos: Gráfico do Traço de Cume (Ridge Trace Plot) e Gráfico do Fator de Inflação da Variância (Variance Inflation Factor Plot).\nUm dos obstáculos principais em utilizar a regressão de cume está em escolher um valor de k. O traço de cume é um esboço dos valores de (p-1) coeficientes estimados de regressão de cume padronizados para diferentes valores de \\(K\\), usualmente entre 0 e 1. Feito o traço, pode-se examinar um valor de \\(K\\) onde as estimativas se estabilizam. Hoerl e Kennard (1970b) desenvolveram um gráfico bidimensional do valor de cada coeficiente versus \\(k\\), mostrando como os valores de \\(\\hat{\\beta}\\) variam em função dos valores de k, ou seja, a partir do gráfico o analista escolhe um valor para K que os coeficientes da regressão tendem a ser mais precisos, que o MQO, quando os dados estão sob o efeito da multicolinearidade.\nSegundo Chagas et al. (2009), o objetivo é escolher um valor de k a partir do qual as estimativas dos parâmetros sejam relativamente estáveis gerando uma série de coeficientes com menor soma dos quadrados do resíduo do que a solução clássica. Assim, na medida em que se aumenta o valor de k, a soma de quadrados dos resíduos também aumentará, sugerindo iniciar com valores pequenos de k e ir aumentando gradativamente até que os coeficientes se estabilizem.\nPara Hoerl e Kennard (1970a), o Fator de Inflação da Variância, mostra a variabilidade em função do valor de K, ou seja, à medida que se atribui valores para K que estabilizam os coeficientes de regressão de cume, a variabilidade diminui, removendo a multicolinearidade. O traço do cume pode também ser usado para sugerir variável(s) que pode(m) ser retiradas do modelo. Algumas variáveis cuja estimativa do parâmetro é instável a cada mudança do valor de K ou que decresce para zero são candidatos para anulação.\nAtualmente na literatura, existem várias medidas corretivas para suavizar os efeitos provocados pela multicolinearidade, outros métodos são propostos desde simples as mais complexas, tais como, Ampliação do Tamanho da Amostra e Remoção das Variáveis (HAIR et al., 2005), utilização de Modelo de Regressão por Componente Principais (SILVA et al., 2009), Modelo de Regressão por Análise Fatorial (VALENTE et al., 2011), Regressão com Variáveis Latentes (MALHOTRA, 2011) e Regressão via Redes Neurais (RODRIGUES et al., 2010) e (LEAL et al., 2015).\nPara rodar a regressão Ridge na linguagem R, usaremos o pacote glmnet.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regressão Ridge</span>"
    ]
  },
  {
    "objectID": "ridge.html#carregar-dataset2",
    "href": "ridge.html#carregar-dataset2",
    "title": "4  Regressão Ridge",
    "section": "4.2 Carregar Dataset2",
    "text": "4.2 Carregar Dataset2\nSegundo Passo é carregar a base de dados, chamada de Volume de Pinus. Para isso é necessário instalar o pacote para leitura de arquivo com extensão do tipo excel(.xlsx) por meio do comando install.packages(“readxl”).\nPosteriormente, ativar o pacote no R com o comando library(readxl). Tendo um detalhe fundamental, que se instala somente um vez o pacote, e se ativa toda vez que for usar.\n\nlibrary(readxl)\n\ndados &lt;- read_excel(\"dados_pinus.xlsx\")\n\n\n4.2.1 Variáveis\nPara ilustrar a regressão ridge, vamos começar com um exemplo em que queremos estudar a relação entre DAP (variável preditora \\(X_{1}\\)) e Volume (variável dependente Y) com uma amostra de 250 arvores.\n\nLocal: Empresa Duratex Florestal SP\nAmostra: 20 arvores\nIAF : Indice Aerea Foliar\nDAF : Distribuição Angular da Folha\nGAP :\nIDADE : Idade em Meses da arvore\nDAP : Diâmetro a Altura do Peito (1.30 metros do solo)\nALTURA : Altura da arvore\nÁEREA BASAL : Area Basal da arvore",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regressão Ridge</span>"
    ]
  },
  {
    "objectID": "ridge.html#matriz-de-correlação",
    "href": "ridge.html#matriz-de-correlação",
    "title": "4  Regressão Ridge",
    "section": "4.3 Matriz de Correlação",
    "text": "4.3 Matriz de Correlação\n\nlibrary(corrplot)\n\ncorrel &lt;- cor(dados[, -1])\ncorrplot(correl, \n         method = \"color\",         # circle, number, shade\n         type = \"upper\",            # lower, full\n         #order = \"hclust\",\n         addCoef.col = \"black\",\n         tl.srt = 90,\n         diag = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regressão Ridge</span>"
    ]
  },
  {
    "objectID": "ridge.html#matriz-de-dispersão",
    "href": "ridge.html#matriz-de-dispersão",
    "title": "4  Regressão Ridge",
    "section": "4.4 Matriz de Dispersão",
    "text": "4.4 Matriz de Dispersão\n\nlibrary(psych)\nlibrary(GGally)\n\npairs.panels(dados,\n             method = \"pearson\",\n             density = TRUE,  \n             ellipses = TRUE,\n             smoother = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regressão Ridge</span>"
    ]
  },
  {
    "objectID": "ridge.html#regressão-linear",
    "href": "ridge.html#regressão-linear",
    "title": "4  Regressão Ridge",
    "section": "4.5 Regressão Linear",
    "text": "4.5 Regressão Linear\nUma das tarefas mais corriqueiras dos analista de dados é a produção de tabelas. Seja para apresentar frequências, estatísticas descritivas (media, mediana, moda, etc), seja para apresentar resultados de modelos de regressão.\nTabela de resultados referente ao modelo de regressão linear multiplo com n =250.\n\nModelo_mlm &lt;- lm(VOLUME ~ ., data = dados)\nsummary(Modelo_mlm)\n\n\nCall:\nlm(formula = VOLUME ~ ., data = dados)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-35.500  -6.329  -0.319   7.374  27.916 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -124.6245    22.3169  -5.584 6.29e-08 ***\nIDADE          8.5913     0.6628  12.962  &lt; 2e-16 ***\nDAP           -4.0900     1.1241  -3.638 0.000335 ***\nALTURA        13.4301     0.2229  60.259  &lt; 2e-16 ***\nIAF            0.6560     1.6718   0.392 0.695108    \nDAF            2.7282     1.3959   1.954 0.051810 .  \nGAP            0.4299     1.5373   0.280 0.779992    \nAREA_BASAL  -227.5270   286.3350  -0.795 0.427614    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.41 on 242 degrees of freedom\nMultiple R-squared:  0.9588,    Adjusted R-squared:  0.9576 \nF-statistic: 803.9 on 7 and 242 DF,  p-value: &lt; 2.2e-16\n\n\n\n4.5.1 Fator de Inflanção da Variância (vIF)\n\nlibrary(car)\n\nvif(Modelo_mlm)  \n\n     IDADE        DAP     ALTURA        IAF        DAF        GAP AREA_BASAL \n  1.044845  65.625225   1.026564   6.652028   1.040344   6.694949  65.782421",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regressão Ridge</span>"
    ]
  },
  {
    "objectID": "ridge.html#modelo-de-regressão-ridge-geral",
    "href": "ridge.html#modelo-de-regressão-ridge-geral",
    "title": "4  Regressão Ridge",
    "section": "4.6 Modelo de Regressão Ridge Geral",
    "text": "4.6 Modelo de Regressão Ridge Geral\n\n#------------------ Pacotes ------------------------------------------#\nlibrary(caret)\nlibrary(glmnet)\n\n#------------------ Modelo Ridge Completo ----------------------------#\nset.seed(123)\ntrain_index &lt;- createDataPartition(dados$VOLUME, p = 0.9, list = FALSE)\ndados_treino &lt;- dados[train_index, ]\ndados_teste  &lt;- dados[-train_index, ]\n\nx_train &lt;- model.matrix(VOLUME ~ ., dados_treino)[, -1]\ny_train &lt;- dados_treino$VOLUME\n\nx_test &lt;- model.matrix(VOLUME ~ ., dados_teste)[, -1]\ny_test &lt;- dados_teste$VOLUME\n\n# Regressão Ridge (Ridge:alpha = 0; LASSO:alpha = 1)\ncv_ridge &lt;- cv.glmnet(x_train,\n                      y_train,\n                      alpha = 0,\n                      standardize = TRUE\n                      )\n# Melhor lambda\nbest_lambda_ridge &lt;- cv_ridge$lambda.min\ncat(\"Melhor lambda (Ridge):\", best_lambda_ridge, \"\\n\")\n\nMelhor lambda (Ridge): 4.345508 \n\nmodelo_ridge &lt;- glmnet(x_train, \n                       y_train,\n                       alpha = 0,\n                       lambda = best_lambda_ridge,\n                       standardize = TRUE)\n\n# Coeficientes do modelo final\ncoef_ridge &lt;- coef(modelo_ridge)\nprint(coef_ridge)\n\n8 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s0\n(Intercept) -112.9141563\nIDADE          7.7791185\nDAP           -2.5523709\nALTURA        12.2855577\nIAF            0.3903256\nDAF            2.3744578\nGAP           -0.0264784\nAREA_BASAL  -601.6109904\n\n# Predição no conjunto de teste\ny_pred_ridge &lt;- as.numeric(predict(modelo_ridge, \n                                   newx = x_test)\n                           )\n#------------------------------------------------------------------------------#\n# Medidas Performance\n\nR2_ridge   &lt;- cor(y_test, y_pred_ridge)^2\nrmse_ridge &lt;- sqrt(mean((y_test - y_pred_ridge)^2))\nmae_ridge  &lt;- mean(abs(y_test - y_pred_ridge))\nmape_ridge &lt;- mean(abs((y_test - y_pred_ridge) / y_test)) * 100\n\n# Predição nos dados de treino para obter o RSS\ny_pred_train_ridge &lt;- predict(modelo_ridge, \n                              s = best_lambda_ridge, \n                              newx = x_train)\n\nRSS &lt;- sum((y_train - y_pred_train_ridge)^2)\nn   &lt;- length(y_train)\n\ndf_ridge &lt;- modelo_ridge$df\n\nAIC_ridge &lt;- n * log(RSS / n) + 2 * (df_ridge + 1)\nBIC_ridge &lt;- n * log(RSS / n) + log(n) * (df_ridge + 1)\n\n\n\ncat(\n  \"\\n--- Performance Ridge Geral ---\\n\",\n  \"R²   =\", round(R2_ridge, 4), \"\\n\",\n  \"RMSE =\", round(rmse_ridge, 4), \"\\n\",\n  \"MAE  =\", round(mae_ridge, 4), \"\\n\",\n  \"MAPE =\", round(mape_ridge, 2), \"%\\n\",\n  \"AIC  =\", round(AIC_ridge, 2), \"\\n\",\n  \"BIC  =\", round(BIC_ridge, 2), \"\\n\"\n)\n\n\n--- Performance Ridge Geral ---\n R²   = 0.9127 \n RMSE = 13.252 \n MAE  = 11.5346 \n MAPE = 26.4 %\n AIC  = 1082.22 \n BIC  = 1109.59 \n\n#------------------------------------------------------------------------------#\n\n\n\n\n\n\n\n4.6.1 Ridge Trace Plot Geral\n\nlibrary(caret)\nlibrary(glmnet)\nlibrary(reshape2)\nlibrary(ggplot2)\n\n\nmodelo_ridge_seq &lt;- glmnet(\n  x_train,\n  y_train,\n  alpha = 0,\n  standardize = TRUE\n)\n\ncoefs &lt;- as.matrix(modelo_ridge_seq$beta)\ndf_coef &lt;- as.data.frame(t(coefs))\ndf_coef$lambda &lt;- log(modelo_ridge_seq$lambda)\n\ndf_long &lt;- melt(\n  df_coef,\n  id.vars = \"lambda\",\n  variable.name = \"Variavel\",\n  value.name = \"Coeficiente\"\n)\n\nggplot(df_long, aes(lambda, Coeficiente, color = Variavel)) +\n  geom_line(linewidth = 2, alpha = 0.85) +\n  geom_vline(\n    xintercept = log(best_lambda_ridge),\n    linetype = \"dashed\",\n    color = \"red\"\n  ) +\n  labs(\n    title = \"Ridge Trace Plot\",\n    subtitle = \"Modelo Ridge Geral\",\n    x = expression(log(lambda)),\n    y = \"Coeficientes\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.6.2 VIF Trace Plot (Geral)\n\nlibrary(ggplot2)\nlibrary(reshape2)\n\n# ---------------- MATRIZ DE PREDITORES ---------------- #\nX &lt;- scale(x_train)\n\n# ---------------- SEQUÊNCIA DE k ---------------- #\nk_values &lt;- exp(seq(\n  log(1e-4),\n  log(10),\n  length.out = 100\n))\n\n# ---------------- FUNÇÃO VIF RIDGE ---------------- #\nridge_vif &lt;- function(X, k) {\n  XtX &lt;- crossprod(X)\n  A &lt;- solve(XtX + diag(k, ncol(X)))\n  diag(A %*% XtX %*% A)\n}\n\n# ---------------- CÁLCULO DOS VIFS ---------------- #\nvif_matrix &lt;- sapply(k_values, function(k) ridge_vif(X, k))\n\n# ---------------- ORGANIZAÇÃO DOS DADOS ---------------- #\nvif_df &lt;- as.data.frame(t(vif_matrix))\ncolnames(vif_df) &lt;- colnames(X)\nvif_df$k &lt;- k_values\n\nvif_long &lt;- melt(\n  vif_df,\n  id.vars = \"k\",\n  variable.name = \"Variavel\",\n  value.name = \"VIF\"\n)\n\n# Remove problemas numéricos\nvif_long &lt;- vif_long[\n  is.finite(vif_long$VIF) & vif_long$VIF &gt; 0,\n]\n\n# ---------------- GRÁFICO FINAL (NORMAL) ---------------- #\nggplot(vif_long,\n       aes(x = log(k),\n           y = VIF,\n           color = Variavel)) +\n  geom_line(linewidth = 1.2, alpha = 0.9) +\n  geom_hline(yintercept = 10,\n             linetype = \"dashed\",\n             color = \"gray40\") +\n  geom_vline(xintercept = log(best_lambda_ridge),\n             linetype = \"dashed\",\n             color = \"red\") +\n  scale_y_log10() +\n  labs(\n    title = \"VIF Trace Plot\",\n    subtitle = \"Modelo Ridge Completo\",\n    x = expression(log(k)),\n    y = \"VIF\",\n    color = \"Variáveis\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 12, color = \"gray40\")\n  )",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regressão Ridge</span>"
    ]
  },
  {
    "objectID": "ridge.html#modelo-de-regressão-ridge-sem-area-basal",
    "href": "ridge.html#modelo-de-regressão-ridge-sem-area-basal",
    "title": "4  Regressão Ridge",
    "section": "4.7 Modelo de Regressão Ridge (Sem Area Basal)",
    "text": "4.7 Modelo de Regressão Ridge (Sem Area Basal)\n\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(glmnet)\nlibrary(car)\nlibrary(corrplot)\nlibrary(ggcorrplot)\nlibrary(ggplot2)\nlibrary(reshape2)\n\n\n## Modelo de Regressão Ridge (Sem Área Basal)\ndados_sem_area &lt;- dados %&gt;% select(-AREA_BASAL)\n\n#---------------------------------------------------------------#\n# Modelo Linear Múltiplo (baseline)\nmodelo2_mlm &lt;- lm(VOLUME ~ ., data = dados_sem_area)\nsummary(modelo2_mlm)\n\n\nCall:\nlm(formula = VOLUME ~ ., data = dados_sem_area)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-35.687  -6.355   0.048   7.512  28.158 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -113.7672    17.6318  -6.452 5.91e-10 ***\nIDADE          8.5571     0.6609  12.948  &lt; 2e-16 ***\nDAP           -4.9760     0.1431 -34.769  &lt; 2e-16 ***\nALTURA        13.4382     0.2225  60.403  &lt; 2e-16 ***\nIAF            0.6504     1.6706   0.389   0.6974    \nDAF            2.7371     1.3948   1.962   0.0509 .  \nGAP            0.4282     1.5362   0.279   0.7807    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.4 on 243 degrees of freedom\nMultiple R-squared:  0.9587,    Adjusted R-squared:  0.9576 \nF-statistic: 939.3 on 6 and 243 DF,  p-value: &lt; 2.2e-16\n\nvif(modelo2_mlm)\n\n   IDADE      DAP   ALTURA      IAF      DAF      GAP \n1.040444 1.065338 1.024446 6.651911 1.040277 6.694936 \n\n#---------------------------------------------------------------#\n# Partição treino / teste\nset.seed(123)\n\ntrain_index2 &lt;- createDataPartition(\n  dados_sem_area$VOLUME,\n  p = 0.9,\n  list = FALSE\n)\n\ndados_treino2 &lt;- dados_sem_area[train_index2, ]\ndados_teste2  &lt;- dados_sem_area[-train_index2, ]\n\n#---------------------------------------------------------------#\n# Matrizes de preditores\nx_train2 &lt;- model.matrix(VOLUME ~ ., dados_treino2)[, -1]\ny_train2 &lt;- dados_treino2$VOLUME\n\nx_test2 &lt;- model.matrix(VOLUME ~ ., dados_teste2)[, -1]\ny_test2 &lt;- dados_teste2$VOLUME\n\n#---------------------------------------------------------------#\n# Ridge com validação cruzada\ncv_ridge2 &lt;- cv.glmnet(\n  x_train2,\n  y_train2,\n  alpha = 0,\n  standardize = TRUE\n)\n\nbest_lambda_ridge2 &lt;- cv_ridge2$lambda.min\n\nmodelo_ridge2 &lt;- glmnet(\n  x_train2,\n  y_train2,\n  alpha = 0,\n  lambda = best_lambda_ridge2,\n  standardize = TRUE\n)\n\n#---------------------------------------------------------------#\n# Predição no conjunto de teste\ny_pred_ridge2 &lt;- as.numeric(\n  predict(modelo_ridge2, newx = x_test2)\n)\n\n#---------------------------------------------------------------#\n# Métricas de desempenho\nR2_ridge2   &lt;- cor(y_test2, y_pred_ridge2)^2\nrmse_ridge2 &lt;- sqrt(mean((y_test2 - y_pred_ridge2)^2))\nmae_ridge2  &lt;- mean(abs(y_test2 - y_pred_ridge2))\nmape_ridge2 &lt;- mean(abs((y_test2 - y_pred_ridge2) / y_test2)) * 100\n\n#---------------------------------------------------------------#\n# AIC e BIC (via RSS + df efetivos)\ny_pred_train_ridge2 &lt;- as.numeric(\n  predict(modelo_ridge2, newx = x_train2)\n)\n\nRSS2 &lt;- sum((y_train2 - y_pred_train_ridge2)^2)\n\nn2  &lt;- length(y_train2)\ndf2 &lt;- modelo_ridge2$df\n\nAIC_ridge2 &lt;- n2 * log(RSS2 / n2) + 2 * (df2 + 1)\nBIC_ridge2 &lt;- n2 * log(RSS2 / n2) + log(n2) * (df2 + 1)\n\n#---------------------------------------------------------------#\n# Resultado final\ncat(\n  \"\\n--- Performance do Modelo Ridge (Sem Área Basal) ---\\n\",\n  \"R²   =\", round(R2_ridge2, 4), \"\\n\",\n  \"RMSE =\", round(rmse_ridge2, 4), \"\\n\",\n  \"MAE  =\", round(mae_ridge2, 4), \"\\n\",\n  \"MAPE =\", round(mape_ridge2, 2), \"%\\n\",\n  \"AIC  =\", round(AIC_ridge2, 2), \"\\n\",\n  \"BIC  =\", round(BIC_ridge2, 2), \"\\n\"\n)\n\n\n--- Performance do Modelo Ridge (Sem Área Basal) ---\n R²   = 0.9215 \n RMSE = 12.7064 \n MAE  = 10.9662 \n MAPE = 25.4 %\n AIC  = 1085.05 \n BIC  = 1109 \n\n\n\n\n\n\n\n\n4.7.1 Ridge Trace Plot (Sem Area Basal)\n\nlibrary(caret)\nlibrary(glmnet)\nlibrary(ggplot2)\nlibrary(reshape2)\n\n### Ridge Trace Plot (Sem Área Basal)\nmodelo_ridge2_seq &lt;- glmnet(\n  x_train2,\n  y_train2,\n  alpha = 0,\n  standardize = TRUE\n)\n\n#---------------------------------------------------------------#\n# Extração dos coeficientes\ncoefs2 &lt;- as.matrix(modelo_ridge2_seq$beta)\nlambdas2 &lt;- modelo_ridge2_seq$lambda\n\ndf_coef2 &lt;- as.data.frame(t(coefs2))\ndf_coef2$lambda &lt;- log(lambdas2)\n\n#---------------------------------------------------------------#\n# Formato longo\ndf_long2 &lt;- melt(\n  df_coef2,\n  id.vars = \"lambda\",\n  variable.name = \"Variavel\",\n  value.name = \"Coeficiente\"\n)\n\n#---------------------------------------------------------------#\n# Ridge Trace Plot\nggplot(df_long2,\n       aes(x = lambda,\n           y = Coeficiente,\n           color = Variavel)) +\n  geom_line(linewidth = 1.2, alpha = 0.9) +\n  geom_vline(\n    xintercept = log(best_lambda_ridge2),\n    linetype = \"dashed\",\n    color = \"red\",\n    linewidth = 1\n  ) +\n  labs(\n    title = \"Ridge Trace Plot\",\n    subtitle = \"Modelo Ridge (Sem Área Basal)\",\n    x = expression(log(lambda)),\n    y = \"Coeficientes dos preditores\"\n  ) +\n  scale_color_brewer(palette = \"Dark2\") +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 13, color = \"gray40\"),\n    legend.title = element_blank(),\n    legend.text = element_text(size = 11),\n    legend.position = \"right\",\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_line(color = \"gray85\")\n  )",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regressão Ridge</span>"
    ]
  },
  {
    "objectID": "ridge.html#modelo-de-regressão-ridge-sem-dap",
    "href": "ridge.html#modelo-de-regressão-ridge-sem-dap",
    "title": "4  Regressão Ridge",
    "section": "4.8 Modelo de Regressão Ridge (Sem DAP)",
    "text": "4.8 Modelo de Regressão Ridge (Sem DAP)\n\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(glmnet)\nlibrary(car)\nlibrary(corrplot)\nlibrary(ggcorrplot)\nlibrary(ggplot2)\nlibrary(reshape2)\n\n\n## Modelo de Regressão Ridge (Sem DAP)\n#---------------------------------------------------------------#\n# Base sem DAP\ndados_sem_dap &lt;- dados %&gt;% select(-DAP)\n\n# Modelo linear auxiliar (diagnóstico)\nmodelo3_mlm &lt;- lm(VOLUME ~ ., data = dados_sem_dap)\nsummary(modelo3_mlm)\n\n\nCall:\nlm(formula = VOLUME ~ ., data = dados_sem_dap)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.457  -6.181  -0.254   7.513  26.999 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -176.1803    17.6702  -9.970   &lt;2e-16 ***\nIDADE           8.7213     0.6783  12.858   &lt;2e-16 ***\nALTURA         13.4068     0.2283  58.718   &lt;2e-16 ***\nIAF             0.7247     1.7133   0.423   0.6727    \nDAF             2.8075     1.4305   1.963   0.0508 .  \nGAP             0.4863     1.5755   0.309   0.7578    \nAREA_BASAL  -1260.8588    37.3898 -33.722   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.67 on 243 degrees of freedom\nMultiple R-squared:  0.9565,    Adjusted R-squared:  0.9554 \nF-statistic: 890.9 on 6 and 243 DF,  p-value: &lt; 2.2e-16\n\nvif(modelo3_mlm)\n\n     IDADE     ALTURA        IAF        DAF        GAP AREA_BASAL \n  1.041809   1.025712   6.651179   1.040091   6.694267   1.067890 \n\n#---------------------------------------------------------------#\n# Divisão treino / teste\nset.seed(123)\ntrain_idx_dap &lt;- createDataPartition(\n  dados_sem_dap$VOLUME,\n  p = 0.9,\n  list = FALSE\n)\n\ndados_treino_dap &lt;- dados_sem_dap[train_idx_dap, ]\ndados_teste_dap  &lt;- dados_sem_dap[-train_idx_dap, ]\n\n#---------------------------------------------------------------#\n# Matrizes para o glmnet\nx_train_dap &lt;- model.matrix(VOLUME ~ ., dados_treino_dap)[, -1]\ny_train_dap &lt;- dados_treino_dap$VOLUME\n\nx_test_dap &lt;- model.matrix(VOLUME ~ ., dados_teste_dap)[, -1]\ny_test_dap &lt;- dados_teste_dap$VOLUME\n\n#---------------------------------------------------------------#\n# Ridge com validação cruzada\ncv_ridge_dap &lt;- cv.glmnet(\n  x_train_dap,\n  y_train_dap,\n  alpha = 0,\n  standardize = TRUE\n)\n\nbest_lambda_dap &lt;- cv_ridge_dap$lambda.min\ncat(\"Melhor lambda (sem DAP):\", best_lambda_dap, \"\\n\")\n\nMelhor lambda (sem DAP): 4.345508 \n\n#---------------------------------------------------------------#\n# Modelo Ridge final\nmodelo_ridge_dap &lt;- glmnet(\n  x_train_dap,\n  y_train_dap,\n  alpha = 0,\n  lambda = best_lambda_dap,\n  standardize = TRUE\n)\n\ncoef(modelo_ridge_dap)\n\n7 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s0\n(Intercept) -1.495238e+02\nIDADE        7.847080e+00\nALTURA       1.231958e+01\nIAF          4.641568e-01\nDAF          2.827680e+00\nGAP          4.234242e-02\nAREA_BASAL  -1.190645e+03\n\n#---------------------------------------------------------------#\n# Predição no conjunto de teste\ny_pred_dap &lt;- predict(\n  modelo_ridge_dap,\n  s = best_lambda_dap,\n  newx = x_test_dap\n)\n\n#---------------------------------------------------------------#\n# Métricas de desempenho\nR2_dap   &lt;- cor(y_test_dap, y_pred_dap)^2\nrmse_dap &lt;- sqrt(mean((y_test_dap - y_pred_dap)^2))\nmae_dap  &lt;- mean(abs(y_test_dap - y_pred_dap))\nmape_dap &lt;- mean(abs((y_test_dap - y_pred_dap) / y_test_dap)) * 100\n\n#---------------------------------------------------------------#\n# AIC e BIC (versão adequada para Ridge)\ny_pred_train_dap &lt;- predict(\n  modelo_ridge_dap,\n  s = best_lambda_dap,\n  newx = x_train_dap\n)\n\nRSS_dap &lt;- sum((y_train_dap - y_pred_train_dap)^2)\n\n# Graus de liberdade efetivos\ndf_dap &lt;- modelo_ridge_dap$df[\n  which(modelo_ridge_dap$lambda == best_lambda_dap)\n]\n\nn_dap &lt;- nrow(dados_treino_dap)\n\nAIC_dap &lt;- n_dap * log(RSS_dap / n_dap) + 2 * (df_dap + 1)\nBIC_dap &lt;- n_dap * log(RSS_dap / n_dap) + log(n_dap) * (df_dap + 1)\n\n#---------------------------------------------------------------#\n# Resultados\ncat(\"\\n--- Performance do Modelo Ridge (Sem DAP) ---\\n\")\n\n\n--- Performance do Modelo Ridge (Sem DAP) ---\n\ncat(\n  \"R²   =\", R2_dap,\n  \"\\nRMSE =\", rmse_dap,\n  \"\\nMAE  =\", mae_dap,\n  \"\\nMAPE =\", mape_dap,\n  \"\\nAIC  =\", AIC_dap,\n  \"\\nBIC  =\", BIC_dap\n)\n\nR²   = 0.9087487 \nRMSE = 13.63578 \nMAE  = 11.86191 \nMAPE = 26.8178 \nAIC  = 1091.824 \nBIC  = 1115.768\n\n\n\n\n\n\n\n\n4.8.1 Ridge Trace Plot (Sem DAP)\n\n### Ridge Trace Plot (Sem DAP)\n\nlibrary(glmnet)\nlibrary(reshape2)\nlibrary(ggplot2)\n\n#---------------------------------------------------------------#\n# Ajuste do caminho completo do Ridge (sem fixar lambda)\nmodelo_ridge_path_dap &lt;- glmnet(\n  x_train_dap,\n  y_train_dap,\n  alpha = 0,\n  standardize = TRUE\n)\n\n#---------------------------------------------------------------#\n# Extração dos coeficientes\ncoefs_dap   &lt;- as.matrix(modelo_ridge_path_dap$beta)\nlambdas_dap &lt;- modelo_ridge_path_dap$lambda\n\ndf_coef_dap &lt;- as.data.frame(t(coefs_dap))\ndf_coef_dap$log_lambda &lt;- log(lambdas_dap)\n\n#---------------------------------------------------------------#\n# Formato longo\ndf_long_dap &lt;- melt(\n  df_coef_dap,\n  id.vars = \"log_lambda\",\n  variable.name = \"Variavel\",\n  value.name = \"Coeficiente\"\n)\n\n#---------------------------------------------------------------#\n# Ridge Trace Plot\nggplot(df_long_dap,\n       aes(x = log_lambda,\n           y = Coeficiente,\n           color = Variavel)) +\n  geom_line(linewidth = 1.2, alpha = 0.9) +\n  geom_vline(\n    xintercept = log(best_lambda_dap),\n    linetype = \"dashed\",\n    linewidth = 1,\n    color = \"red\"\n  ) +\n  labs(\n    title = \"Ridge Trace Plot\",\n    subtitle = \"Modelo Ridge (Sem DAP)\",\n    x = expression(log(lambda)),\n    y = \"Coeficientes dos preditores\",\n    color = \"Variáveis\"\n  ) +\n  scale_color_brewer(palette = \"Dark2\") +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 13, color = \"gray40\"),\n    legend.title = element_text(size = 11),\n    legend.text = element_text(size = 10),\n    legend.position = \"right\",\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_line(color = \"gray85\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.8.2 VIF Trace Plot (Sem DAP)\n\n### VIF Trace Plot (Sem DAP)\n\nlibrary(ggplot2)\nlibrary(reshape2)\n\n#---------------------------------------------------------------#\n# MATRIZ DE PREDITORES (SEM DAP)\nX_dap &lt;- scale(x_train_dap)\n\n#---------------------------------------------------------------#\n# SEQUÊNCIA DE k (penalização)\nk_values &lt;- exp(seq(\n  log(1e-4),\n  log(10),\n  length.out = 100\n))\n\n#---------------------------------------------------------------#\n# FUNÇÃO PARA CÁLCULO DO VIF RIDGE\nridge_vif &lt;- function(X, k) {\n  XtX &lt;- crossprod(X)\n  A &lt;- solve(XtX + diag(k, ncol(X)))\n  diag(A %*% XtX %*% A)\n}\n\n#---------------------------------------------------------------#\n# CÁLCULO DOS VIFS AO LONGO DE k\nvif_matrix_dap &lt;- sapply(\n  k_values,\n  function(k) ridge_vif(X_dap, k)\n)\n\n#---------------------------------------------------------------#\n# ORGANIZAÇÃO DOS DADOS\nvif_df_dap &lt;- as.data.frame(t(vif_matrix_dap))\ncolnames(vif_df_dap) &lt;- colnames(X_dap)\nvif_df_dap$k &lt;- k_values\n\nvif_long_dap &lt;- melt(\n  vif_df_dap,\n  id.vars = \"k\",\n  variable.name = \"Variavel\",\n  value.name = \"VIF\"\n)\n\n# Remove problemas numéricos\nvif_long_dap &lt;- vif_long_dap[\n  is.finite(vif_long_dap$VIF) & vif_long_dap$VIF &gt; 0,\n]\n\n#---------------------------------------------------------------#\n# GRÁFICO VIF TRACE PLOT\nggplot(vif_long_dap,\n       aes(x = log(k),\n           y = VIF,\n           color = Variavel)) +\n  geom_line(linewidth = 1.2, alpha = 0.9) +\n  geom_hline(\n    yintercept = 10,\n    linetype = \"dashed\",\n    color = \"gray40\"\n  ) +\n  geom_vline(\n    xintercept = log(best_lambda_dap),\n    linetype = \"dashed\",\n    color = \"red\",\n    linewidth = 1\n  ) +\n  scale_y_log10() +\n  labs(\n    title = \"VIF Trace Plot\",\n    subtitle = \"Modelo Ridge (Sem DAP)\",\n    x = expression(log(k)),\n    y = \"VIF\",\n    color = \"Variáveis\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 12, color = \"gray40\"),\n    legend.title = element_text(size = 11),\n    legend.text = element_text(size = 10)\n  )",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regressão Ridge</span>"
    ]
  },
  {
    "objectID": "ridge.html#modelo-de-regressão-ridge-sem-dap-area-basal",
    "href": "ridge.html#modelo-de-regressão-ridge-sem-dap-area-basal",
    "title": "4  Regressão Ridge",
    "section": "4.9 Modelo de Regressão Ridge (Sem DAP + Area Basal)",
    "text": "4.9 Modelo de Regressão Ridge (Sem DAP + Area Basal)\n\n## Modelo de Regressão Ridge (Sem DAP + Área Basal)\n\nlibrary(dplyr)\nlibrary(caret)\nlibrary(glmnet)\n\n#---------------------------------------------------------------#\n# REMOÇÃO DAS VARIÁVEIS\ndados_sem_dap_ab &lt;- dados %&gt;% \n  select(-DAP, -AREA_BASAL)\n\n#---------------------------------------------------------------#\n# DIVISÃO TREINO / TESTE\nset.seed(123)\ntrain_idx_dap_ab &lt;- createDataPartition(\n  dados_sem_dap_ab$VOLUME,\n  p = 0.9,\n  list = FALSE\n)\n\ndados_treino_dap_ab &lt;- dados_sem_dap_ab[train_idx_dap_ab, ]\ndados_teste_dap_ab  &lt;- dados_sem_dap_ab[-train_idx_dap_ab, ]\n\n#---------------------------------------------------------------#\n# MATRIZES DE PROJETO\nx_train_dap_ab &lt;- model.matrix(\n  VOLUME ~ ., \n  dados_treino_dap_ab\n)[, -1]\n\ny_train_dap_ab &lt;- dados_treino_dap_ab$VOLUME\n\nx_test_dap_ab &lt;- model.matrix(\n  VOLUME ~ ., \n  dados_teste_dap_ab\n)[, -1]\n\ny_test_dap_ab &lt;- dados_teste_dap_ab$VOLUME\n\n#---------------------------------------------------------------#\n# RIDGE COM VALIDAÇÃO CRUZADA\ncv_ridge_dap_ab &lt;- cv.glmnet(\n  x_train_dap_ab,\n  y_train_dap_ab,\n  alpha = 0,\n  standardize = TRUE\n)\n\nbest_lambda_dap_ab &lt;- cv_ridge_dap_ab$lambda.min\ncat(\n  \"Melhor lambda (sem DAP e Área Basal):\",\n  best_lambda_dap_ab, \"\\n\"\n)\n\nMelhor lambda (sem DAP e Área Basal): 4.345508 \n\n#---------------------------------------------------------------#\n# MODELO FINAL\nmodelo_ridge_dap_ab &lt;- glmnet(\n  x_train_dap_ab,\n  y_train_dap_ab,\n  alpha = 0,\n  lambda = best_lambda_dap_ab,\n  standardize = TRUE\n)\n\n#---------------------------------------------------------------#\n# PREDIÇÃO (TESTE)\ny_pred_dap_ab &lt;- predict(\n  modelo_ridge_dap_ab,\n  s = best_lambda_dap_ab,\n  newx = x_test_dap_ab\n)\n\n#---------------------------------------------------------------#\n# MÉTRICAS DE DESEMPENHO\nR2_dap_ab   &lt;- cor(y_test_dap_ab, y_pred_dap_ab)^2\nrmse_dap_ab &lt;- sqrt(mean((y_test_dap_ab - y_pred_dap_ab)^2))\nmae_dap_ab  &lt;- mean(abs(y_test_dap_ab - y_pred_dap_ab))\nmape_dap_ab &lt;- mean(\n  abs((y_test_dap_ab - y_pred_dap_ab) / y_test_dap_ab)\n) * 100\n\n#---------------------------------------------------------------#\n# AIC e BIC (APROXIMADOS)\ny_pred_train_dap_ab &lt;- predict(\n  modelo_ridge_dap_ab,\n  s = best_lambda_dap_ab,\n  newx = x_train_dap_ab\n)\n\nRSS_dap_ab &lt;- sum(\n  (y_train_dap_ab - y_pred_train_dap_ab)^2\n)\n\ndf_dap_ab &lt;- modelo_ridge_dap_ab$df[\n  which.min(\n    abs(modelo_ridge_dap_ab$lambda - best_lambda_dap_ab)\n  )\n]\n\nn_dap_ab &lt;- nrow(dados_treino_dap_ab)\n\nAIC_dap_ab &lt;- n_dap_ab * log(RSS_dap_ab / n_dap_ab) +\n  2 * (df_dap_ab + 1)\n\nBIC_dap_ab &lt;- n_dap_ab * log(RSS_dap_ab / n_dap_ab) +\n  log(n_dap_ab) * (df_dap_ab + 1)\n\n#---------------------------------------------------------------#\n# RESULTADOS\ncat(\"\\n--- Modelo Ridge SEM DAP e SEM Área Basal ---\\n\")\n\n\n--- Modelo Ridge SEM DAP e SEM Área Basal ---\n\ncat(\n  \"R² =\", R2_dap_ab,\n  \"\\nRMSE =\", rmse_dap_ab,\n  \"\\nMAE =\", mae_dap_ab,\n  \"\\nMAPE =\", mape_dap_ab,\n  \"\\nAIC =\", AIC_dap_ab,\n  \"\\nBIC =\", BIC_dap_ab\n)\n\nR² = 0.713538 \nRMSE = 22.66082 \nMAE = 16.85935 \nMAPE = 31.39215 \nAIC = 1476.926 \nBIC = 1497.45\n\n\n\n\n\n\n\n\n4.9.1 Ridge Trace Plot (Sem DAP + Area Basal)\n\nlibrary(glmnet)\nlibrary(reshape2)\nlibrary(ggplot2)\n\n# Ajuste do caminho completo do Ridge\nmodelo_ridge_dap_ab &lt;- glmnet(\n  x_train_dap_ab,\n  y_train_dap_ab,\n  alpha = 0,\n  standardize = TRUE\n)\n\n# Extração dos coeficientes\ncoefs_dap_ab &lt;- as.matrix(modelo_ridge_dap_ab$beta)\nlambdas_dap_ab &lt;- modelo_ridge_dap_ab$lambda\n\n# Organização dos dados\ndf_coef_dap_ab &lt;- as.data.frame(t(coefs_dap_ab))\ndf_coef_dap_ab$log_lambda &lt;- log(lambdas_dap_ab)\n\ndf_long_dap_ab &lt;- melt(\n  df_coef_dap_ab,\n  id.vars = \"log_lambda\",\n  variable.name = \"Variavel\",\n  value.name = \"Coeficiente\"\n)\n\n# Gráfico Ridge Trace Plot\nggplot(df_long_dap_ab,\n       aes(x = log_lambda, y = Coeficiente, color = Variavel)) +\n  geom_line(linewidth = 1.2, alpha = 0.9) +\n  geom_vline(\n    xintercept = log(best_lambda_dap_ab),\n    linetype = \"dashed\",\n    linewidth = 1,\n    color = \"red\"\n  ) +\n  labs(\n    title = \"Ridge Trace Plot\",\n    subtitle = \"Modelo Ridge (Sem DAP e Área Basal)\",\n    x = expression(log(lambda)),\n    y = \"Coeficientes dos preditores\"\n  ) +\n  scale_color_brewer(palette = \"Dark2\") +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 13, color = \"gray40\"),\n    legend.position = \"right\",\n    legend.title = element_blank(),\n    legend.text = element_text(size = 11),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_line(color = \"gray85\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.9.2 VIF Trace Plot (Sem DAP + Area Basal)\n\nlibrary(ggplot2)\nlibrary(reshape2)\n\n# ---------------- MATRIZ DE PREDITORES ---------------- #\nX &lt;- scale(x_train_dap_ab)\n\n# ---------------- SEQUÊNCIA LOGARÍTMICA DE k ---------------- #\nk_values &lt;- exp(seq(\n  log(1e-4),\n  log(10),\n  length.out = 100\n))\n\n# ---------------- FUNÇÃO VIF RIDGE ---------------- #\nridge_vif &lt;- function(X, k) {\n  XtX &lt;- crossprod(X)\n  A &lt;- solve(XtX + diag(k, ncol(X)))\n  diag(A %*% XtX %*% A)\n}\n\n# ---------------- CÁLCULO DOS VIFS ---------------- #\nvif_matrix &lt;- sapply(k_values, function(k) ridge_vif(X, k))\n\n# ---------------- ORGANIZAÇÃO DOS DADOS ---------------- #\nvif_df &lt;- as.data.frame(t(vif_matrix))\ncolnames(vif_df) &lt;- colnames(X)\nvif_df$log_k &lt;- log(k_values)\n\nvif_long &lt;- melt(\n  vif_df,\n  id.vars = \"log_k\",\n  variable.name = \"Variavel\",\n  value.name = \"VIF\"\n)\n\n# Remoção de problemas numéricos\nvif_long &lt;- vif_long[\n  is.finite(vif_long$VIF) & vif_long$VIF &gt; 0,\n]\n\n# ---------------- GRÁFICO FINAL ---------------- #\nggplot(\n  vif_long,\n  aes(x = log_k, y = VIF, color = Variavel, group = Variavel)\n) +\n  geom_line(linewidth = 1.5, alpha = 0.9) +\n  scale_y_log10() +\n  geom_hline(\n    yintercept = 10,\n    linetype = \"dashed\",\n    color = \"gray40\"\n  ) +\n  geom_vline(\n    xintercept = log(best_lambda_dap_ab),\n    linetype = \"dashed\",\n    linewidth = 1,\n    color = \"red\"\n  ) +\n  labs(\n    title = \"VIF Trace Plot\",\n    subtitle = \"Modelo Ridge (Sem DAP e Área Basal)\",\n    x = expression(log(k)),\n    y = \"VIF Ridge\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 13, color = \"gray40\"),\n    legend.position = \"right\",\n    legend.title = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.9.3 Comparação dos Modelos\n\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(kableExtra)\n\ntabela_ridge &lt;- tibble(\n  Modelo = c(\n    \"Ridge Completo\",\n    \"Ridge sem Área Basal\",\n    \"Ridge sem DAP\",\n    \"Ridge sem DAP e Área Basal\"\n  ),\n  Lambda_Otimo = c(\n    best_lambda_ridge,\n    best_lambda_ridge2,\n    best_lambda_dap,\n    best_lambda_dap_ab\n  ),\n  R2 = c(\n    R2_ridge,\n    R2_ridge2,\n    R2_dap,\n    R2_dap_ab\n  ),\n  RMSE = c(\n    rmse_ridge,\n    rmse_ridge2,\n    rmse_dap,\n    rmse_dap_ab\n  ),\n  MAE = c(\n    mae_ridge,\n    mae_ridge2,\n    mae_dap,\n    mae_dap_ab\n  ),\n  MAPE = c(\n    mape_ridge,\n    mape_ridge2,\n    mape_dap,\n    mape_dap_ab\n  ),\n  AIC = c(\n    AIC_ridge,\n    AIC_ridge2,\n    AIC_dap,\n    AIC_dap_ab\n  ),\n  BIC = c(\n    BIC_ridge,\n    BIC_ridge2,\n    BIC_dap,\n    BIC_dap_ab\n  )\n) %&gt;%\n  mutate(\n    Lambda_Otimo = round(Lambda_Otimo, 5),\n    R2 = round(R2, 4),\n    RMSE = round(RMSE, 3),\n    MAE = round(MAE, 3),\n    MAPE = round(MAPE, 2),\n    AIC = round(AIC, 2),\n    BIC = round(BIC, 2)\n  )\n\nkable(\n  tabela_ridge,\n  caption = \"Comparação dos modelos Ridge\",\n  align = \"c\"\n) %&gt;%\n  kable_styling(\n    full_width = FALSE,\n    position = \"center\",\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\")\n  )\n\n\nComparação dos modelos Ridge\n\n\nModelo\nLambda_Otimo\nR2\nRMSE\nMAE\nMAPE\nAIC\nBIC\n\n\n\n\nRidge Completo\n4.34551\n0.9127\n13.252\n11.535\n26.40\n1082.22\n1109.59\n\n\nRidge sem Área Basal\n4.34551\n0.9215\n12.706\n10.966\n25.40\n1085.05\n1109.00\n\n\nRidge sem DAP\n4.34551\n0.9087\n13.636\n11.862\n26.82\n1091.82\n1115.77\n\n\nRidge sem DAP e Área Basal\n4.34551\n0.7135\n22.661\n16.859\n31.39\n1476.93\n1497.45\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(ggrepel)\n\ndf_comp &lt;- data.frame(\n  Modelo = c(\n    \"Geral\",\n    \"Sem Área Basal\",\n    \"Sem DAP\",\n    \"Sem DAP/Área Basal\"\n  ),\n  RMSE = c(\n    rmse_ridge,\n    rmse_ridge2,\n    rmse_dap,\n    rmse_dap_ab\n  ),\n  AIC = c(\n    AIC_ridge,\n    AIC_ridge2,\n    AIC_dap,\n    AIC_dap_ab\n  )\n)\n\nggplot(df_comp, aes(x = AIC, y = RMSE, label = Modelo)) +\n  geom_point(size = 4, color = \"firebrick\") +\n  geom_text_repel(size = 4, box.padding = 0.4) +\n  labs(\n    title = \"Comparação dos Modelos Ridge\",\n    subtitle = \"\",\n    x = \"AIC\",\n    y = \"RMSE\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.9.4 Modelo Elastic Net (Geral)\n\n## =========================================================\n## Elastic Net – Modelo Geral (Ridge + LASSO)\n## =========================================================\n\nlibrary(caret)\nlibrary(glmnet)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\nset.seed(123)\n\n#--------------------------------------------\n# Dados de treino e teste (mesmo padrão)\n#--------------------------------------------\ntrain_index_enet &lt;- createDataPartition(dados$VOLUME, p = 0.9, list = FALSE)\ndados_treino_enet &lt;- dados[train_index_enet, ]\ndados_teste_enet  &lt;- dados[-train_index_enet, ]\n\nx_train_enet &lt;- model.matrix(VOLUME ~ ., dados_treino_enet)[, -1]\ny_train_enet &lt;- dados_treino_enet$VOLUME\n\nx_test_enet &lt;- model.matrix(VOLUME ~ ., dados_teste_enet)[, -1]\ny_test_enet &lt;- dados_teste_enet$VOLUME\n\n\n#--------------------------------------------\n# Grid PODEROSO de alpha e lambda\n#--------------------------------------------\nalpha_grid &lt;- seq(0.1, 0.9, by = 0.1)\n\n\nset.seed(123)\n\ncv_results &lt;- lapply(alpha_grid, function(a) {\n  \n  cv &lt;- cv.glmnet(\n    x_train_enet,\n    y_train_enet,\n    alpha = a,\n    standardize = TRUE,\n    nfolds = 10\n  )\n  \n  data.frame(\n    alpha = a,\n    lambda = cv$lambda.min,\n    cvm = min(cv$cvm)\n  )\n})\n\ncv_df &lt;- do.call(rbind, cv_results)\n\n\n\nbest_row &lt;- cv_df[which.min(cv_df$cvm), ]\n\nbest_alpha_enet  &lt;- best_row$alpha\nbest_lambda_enet &lt;- best_row$lambda\n\ncat(\"Alpha ótimo:\", best_alpha_enet, \"\\n\")\n\nAlpha ótimo: 0.5 \n\ncat(\"Lambda ótimo:\", best_lambda_enet, \"\\n\")\n\nLambda ótimo: 0.2475232 \n\nmodelo_enet &lt;- glmnet(\n  x_train_enet,\n  y_train_enet,\n  alpha = best_alpha_enet,\n  lambda = best_lambda_enet,\n  standardize = TRUE\n)\n\ny_pred_enet &lt;- predict(modelo_enet, newx = x_test_enet)\n\nR2_enet   &lt;- cor(y_test_enet, y_pred_enet)^2\nrmse_enet &lt;- sqrt(mean((y_test_enet - y_pred_enet)^2))\nmae_enet  &lt;- mean(abs(y_test_enet - y_pred_enet))\nmape_enet &lt;- mean(abs((y_test_enet - y_pred_enet) / y_test_enet)) * 100\n\ncoef_enet &lt;- coef(modelo_enet)\n\nvariaveis_enet &lt;- rownames(coef_enet)[\n  coef_enet[, 1] != 0 & rownames(coef_enet) != \"(Intercept)\"\n]\n\nn &lt;- length(y_train_enet)\n\ny_fitted_enet &lt;- predict(modelo_enet, newx = x_train_enet)\n\nrss_enet &lt;- sum((y_train_enet - y_fitted_enet)^2)\nsigma2_enet &lt;- rss_enet / n\n\nlogLik_enet &lt;- -n/2 * (log(2 * pi) + log(sigma2_enet) + 1)\n\ndf_enet &lt;- modelo_enet$df\n\nAIC_enet &lt;- -2 * logLik_enet + 2 * df_enet\nBIC_enet &lt;- -2 * logLik_enet + log(n) * df_enet\n\ncat(\"\\n==============================\\n\")\n\n\n==============================\n\ncat(\"ELASTIC NET — MODELO FINAL\\n\")\n\nELASTIC NET — MODELO FINAL\n\ncat(\"==============================\\n\")\n\n==============================\n\ncat(\"Alpha ótimo:\", best_alpha_enet,\n    \"\\nLambda ótimo:\", best_lambda_enet,\n    \"\\nR²:\", R2_enet,\n    \"\\nRMSE:\", rmse_enet,\n    \"\\nMAE:\", mae_enet,\n    \"\\nMAPE:\", mape_enet,\n    \"\\nAIC:\", AIC_enet,\n    \"\\nBIC:\", BIC_enet,\n    \"\\n\\nVariáveis selecionadas:\\n\",\n    paste(variaveis_enet, collapse = \", \"),\n    \"\\n\")\n\nAlpha ótimo: 0.5 \nLambda ótimo: 0.2475232 \nR²: 0.9188018 \nRMSE: 12.29131 \nMAE: 10.54561 \nMAPE: 23.6061 \nAIC: 1695.341 \nBIC: 1715.864 \n\nVariáveis selecionadas:\n IDADE, DAP, ALTURA, DAF, GAP, AREA_BASAL",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regressão Ridge</span>"
    ]
  },
  {
    "objectID": "lasso.html",
    "href": "lasso.html",
    "title": "5  Regressão LASSO",
    "section": "",
    "text": "5.0.1 Variáveis\nA regressçao ridge mantém todas as variáveis no modelo final, memso quando \\(k\\) é grande. Isso pode ser um problema na interpretação do modelo, por conta da quantidade de variáveis e pelos baixos coeficientes. O lasso pe uma alternativa à regressão ridge que permite obter um modelo final com um subconjunto de variáveis.\nPara rodar a regressão Lasso na linguagem R, usaremos o pacote glmnet.\nPara ilustrar a regressão ridge, vamos começar com um exemplo em que queremos estudar a relação entre DAP (variável preditora \\(X_{1}\\)) e Volume (variável dependente Y) com uma amostra de 250 arvores.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regressão LASSO</span>"
    ]
  },
  {
    "objectID": "lasso.html#modelo-regressão-lasso-geral",
    "href": "lasso.html#modelo-regressão-lasso-geral",
    "title": "5  Regressão LASSO",
    "section": "5.1 Modelo Regressão LASSO Geral",
    "text": "5.1 Modelo Regressão LASSO Geral\n\n## Modelo Regressão LASSO Geral\n\nlibrary(caret)\nlibrary(glmnet)\n\nset.seed(123)\ntrain_index_lasso &lt;- createDataPartition(dados$VOLUME, p = 0.9, list = FALSE)\ndados_treino_lasso &lt;- dados[train_index_lasso, ]\ndados_teste_lasso  &lt;- dados[-train_index_lasso, ]\n\nx_train_lasso &lt;- model.matrix(VOLUME ~ ., dados_treino_lasso)[, -1]\ny_train_lasso &lt;- dados_treino_lasso$VOLUME\n\nx_test_lasso &lt;- model.matrix(VOLUME ~ ., dados_teste_lasso)[, -1]\ny_test_lasso &lt;- dados_teste_lasso$VOLUME\n\n# Validação cruzada LASSO\ncv_lasso &lt;- cv.glmnet(\n  x_train_lasso,\n  y_train_lasso,\n  alpha = 1,\n  standardize = TRUE\n)\n\nbest_lambda_lasso &lt;- cv_lasso$lambda.min\n\nmodelo_lasso &lt;- glmnet(\n  x_train_lasso,\n  y_train_lasso,\n  alpha = 1,\n  lambda = best_lambda_lasso,\n  standardize = TRUE\n)\n\n# Predição\ny_pred_lasso &lt;- predict(modelo_lasso, newx = x_test_lasso)\n\n# Métricas\nR2_lasso &lt;- cor(y_test_lasso, y_pred_lasso)^2\nrmse_lasso &lt;- sqrt(mean((y_test_lasso - y_pred_lasso)^2))\nmae_lasso &lt;- mean(abs(y_test_lasso - y_pred_lasso))\nmape_lasso &lt;- mean(abs((y_test_lasso - y_pred_lasso) / y_test_lasso)) * 100\n\n# Variáveis selecionadas\ncoef_lasso &lt;- coef(modelo_lasso)\nvariaveis_lasso &lt;- rownames(coef_lasso)[\n  coef_lasso[, 1] != 0 & rownames(coef_lasso) != \"(Intercept)\"\n]\n\n# ---------------- AIC e BIC ---------------- #\ny_fitted_lasso &lt;- predict(modelo_lasso, newx = x_train_lasso)\nRSS_lasso &lt;- sum((y_train_lasso - y_fitted_lasso)^2)\n\ndf_lasso &lt;- modelo_lasso$df[modelo_lasso$lambda == best_lambda_lasso]\nn &lt;- length(y_train_lasso)\n\nAIC_lasso &lt;- n * log(RSS_lasso / n) + 2 * (df_lasso + 1)\nBIC_lasso &lt;- n * log(RSS_lasso / n) + log(n) * (df_lasso + 1)\n\n# Resultados\ncat(\"\\n--- Modelo LASSO  ---\\n\")\n\n\n--- Modelo LASSO  ---\n\ncat(\n  \"R² =\", R2_lasso,\n  \"\\nRMSE =\", rmse_lasso,\n  \"\\nMAE =\", mae_lasso,\n  \"\\nMAPE =\", mape_lasso,\n  \"\\nAIC =\", AIC_lasso,\n  \"\\nBIC =\", BIC_lasso\n)\n\nR² = 0.9199676 \nRMSE = 12.2659 \nMAE = 10.5483 \nMAPE = 23.57098 \nAIC = 1055.38 \nBIC = 1075.903\n\ncat(\n  \"\\nVariáveis selecionadas:\",\n  paste(variaveis_lasso, collapse = \", \"),\n  \"\\n\"\n)\n\n\nVariáveis selecionadas: IDADE, DAP, ALTURA, DAF, AREA_BASAL",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regressão LASSO</span>"
    ]
  },
  {
    "objectID": "lasso.html#lasso-trace-plot",
    "href": "lasso.html#lasso-trace-plot",
    "title": "5  Regressão LASSO",
    "section": "5.2 Lasso Trace Plot",
    "text": "5.2 Lasso Trace Plot\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(glmnet)\n\n\nlasso_seq &lt;- glmnet(x_train_lasso, \n                    y_train_lasso, \n                    alpha = 1, \n                    standardize = TRUE)\n\n# Índice do lambda ótimo\nid_best &lt;- which.min(abs(lasso_seq$lambda - best_lambda_lasso))\n\n# Coeficientes no lambda ótimo\ncoef_best &lt;- as.matrix(lasso_seq$beta)[, id_best]\n\n# Variáveis selecionadas (≠ 0)\nvars_selected &lt;- names(coef_best[coef_best != 0])\n\n# Construir data frame dos coeficientes ao longo do caminho\ndf_lasso_path &lt;- as.data.frame(t(as.matrix(lasso_seq$beta)))\ndf_lasso_path$log_lambda &lt;- log(lasso_seq$lambda)\n\n# Manter apenas variáveis selecionadas no modelo final\ndf_lasso_path &lt;- df_lasso_path %&gt;%\n  select(log_lambda, all_of(vars_selected)) %&gt;%\n  pivot_longer(\n    cols = -log_lambda,\n    names_to = \"Variavel\",\n    values_to = \"Coeficiente\"\n  )\n\n# Gráfico LASSO TRACE – MODELO ESCOLHIDO\nggplot(\n  df_lasso_path,\n  aes(x = log_lambda, y = Coeficiente, color = Variavel)\n) +\n  geom_line(size = 1.3) +\n  geom_vline(\n    xintercept = log(best_lambda_lasso),\n    linetype = \"dashed\",\n    linewidth = 1,\n    color = \"black\"\n  ) +\n  labs(\n    title = \"LASSO Trace Plot\",\n    subtitle = expression(paste(\n      \"\"\n    )),\n    x = expression(log(lambda)),\n    y = \"Coeficiente\"\n  ) +\n  scale_color_brewer(palette = \"Dark2\") +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 13, color = \"gray40\"),\n    legend.title = element_blank(),\n    panel.grid.minor = element_blank()\n  )",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regressão LASSO</span>"
    ]
  },
  {
    "objectID": "lasso.html#modelo-mlp-rede-neural",
    "href": "lasso.html#modelo-mlp-rede-neural",
    "title": "5  Regressão LASSO",
    "section": "5.3 MODELO MLP (REDE NEURAL)",
    "text": "5.3 MODELO MLP (REDE NEURAL)\n\nlibrary(caret)\nlibrary(nnet)\nlibrary(dplyr)\n\n\nset.seed(123)\n\ntrain_index_nn &lt;- createDataPartition(dados$VOLUME, p = 0.9, list = FALSE)\n\ndados_treino_nn &lt;- dados[train_index_nn, ]\ndados_teste_nn  &lt;- dados[-train_index_nn, ]\n\npreproc &lt;- preProcess(dados_treino_nn[, -which(names(dados) == \"VOLUME\")],\n                      method = c(\"center\", \"scale\"))\n\nx_train_nn &lt;- predict(preproc, dados_treino_nn)\nx_test_nn  &lt;- predict(preproc, dados_teste_nn)\n\n\n\nset.seed(123)\n\nctrl &lt;- trainControl(method = \"cv\", number = 10)\n\ngrid_mlp &lt;- expand.grid(\n  size = c(3, 5, 7),\n  decay = c(0.001, 0.01, 0.1)\n)\n\nmodelo_mlp &lt;- train(\n  VOLUME ~ .,\n  data = x_train_nn,\n  method = \"nnet\",\n  tuneGrid = grid_mlp,\n  trControl = ctrl,\n  linout = TRUE,\n  trace = FALSE,\n  maxit = 500\n)\n\n\ny_pred_mlp &lt;- predict(modelo_mlp, newdata = x_test_nn)\n\nR2_mlp   &lt;- cor(dados_teste_nn$VOLUME, y_pred_mlp)^2\nrmse_mlp &lt;- RMSE(y_pred_mlp, dados_teste_nn$VOLUME)\nmae_mlp  &lt;- MAE(y_pred_mlp, dados_teste_nn$VOLUME)\n\n\n\ncat(\"\\n--- REDE NEURAL (MLP) ---\\n\")\n\n\n--- REDE NEURAL (MLP) ---\n\ncat(\"R²:\", R2_mlp,\n    \"\\nRMSE:\", rmse_mlp,\n    \"\\nMAE:\", mae_mlp,\n    \"\\nMelhores parâmetros:\",\n    \"\\nNeurônios ocultos:\", modelo_mlp$bestTune$size,\n    \"\\nDecay:\", modelo_mlp$bestTune$decay,\n    \"\\n\")\n\nR²: 0.9180618 \nRMSE: 12.37432 \nMAE: 10.42029 \nMelhores parâmetros: \nNeurônios ocultos: 3 \nDecay: 0.1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regressão LASSO</span>"
    ]
  },
  {
    "objectID": "randonforest.html",
    "href": "randonforest.html",
    "title": "6  Randon Forest",
    "section": "",
    "text": "library(readxl)\n\ndados &lt;- read_excel(\"dados_pinus.xlsx\")\n\n\nlibrary(randomForest)\nlibrary(caret)\nlibrary(magrittr)\nlibrary(dplyr)\n\ndados_rf &lt;- dados %&gt;% select(-AREA_BASAL)\n\n# 3. Divisão dos Dados em Treino e Teste (usando a mesma metodologia)\nset.seed(123) # Para reprodutibilidade\ntrain_index_rf &lt;- createDataPartition(dados_rf$VOLUME, p = 0.9, list = FALSE)\ndados_treino_rf &lt;- dados_rf[train_index_rf, ]\ndados_teste_rf &lt;- dados_rf[-train_index_rf, ]\n\n# Separar a variável resposta (y) e as preditoras (x) no teste\ny_test_rf &lt;- dados_teste_rf$VOLUME\nx_test_rf &lt;- dados_teste_rf %&gt;% select(-VOLUME)\n\n\n# 4. Treinamento do Modelo Random Forest\n# ntree: Número de árvores na floresta (um valor entre 500-1000 é um bom começo).\n# mtry: Número de variáveis testadas em cada \"nó\" da árvore. O padrão para regressão é (nº de variáveis / 3).\nset.seed(123) # Para reprodutibilidade do modelo\nmodelo_rf &lt;- randomForest(\n  VOLUME ~ .,                  # Fórmula: prever VOLUME usando todas as outras variáveis\n  data = dados_treino_rf,      # Usar os dados de treino\n  ntree = 500,                 # Número de árvores\n  importance = TRUE            # Guardar a importância das variáveis\n)\n\n# 5. Visualizar o resultado do modelo\nprint(modelo_rf)\n\n\nCall:\n randomForest(formula = VOLUME ~ ., data = dados_treino_rf, ntree = 500,      importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n          Mean of squared residuals: 372.5155\n                    % Var explained: 85.8\n\n# O resultado mostrará o \"Mean of squared residuals\" (MSE) e \"% Var explained\" (R²)\n# calculados internamente nos dados de \"out-of-bag\" (uma forma de validação cruzada)\n\n\n# 6. Fazer Previsões no Conjunto de Teste\ny_pred_rf &lt;- predict(modelo_rf, newdata = x_test_rf)\n\n\n# 7. Avaliar a Performance do Modelo (comparando com o seu Ridge)\nR2_rf &lt;- cor(y_test_rf, y_pred_rf)^2\nrmse_rf &lt;- sqrt(mean((y_test_rf - y_pred_rf)^2))\nmae_rf &lt;- mean(abs(y_test_rf - y_pred_rf))\nmape_rf &lt;- mean(abs((y_test_rf - y_pred_rf) / y_test_rf)) * 100\n\n# 8. Apresentar os resultados\ncat(\"\\n--- Performance do Modelo Random Forest ---\\n\")\n\n\n--- Performance do Modelo Random Forest ---\n\ncat(\"R² =\", R2_rf, \n    \"\\nRMSE =\", rmse_rf, \n    \"\\nMAE =\", mae_rf, \n    \"\\nMAPE =\", mape_rf\n    )\n\nR² = 0.8570756 \nRMSE = 18.14786 \nMAE = 15.15323 \nMAPE = 30.8996\n\n# Ver a importância das variáveis\nimportancia &lt;- importance(modelo_rf)\nprint(importancia)\n\n         %IncMSE IncNodePurity\nIDADE   4.076462      40717.96\nDAP    43.316461     144179.19\nALTURA 67.149910     302520.23\nIAF     4.323521      27230.97\nDAF     1.848441      33294.95\nGAP     4.799672      25632.07\n\n# Criar um gráfico de importância\nvarImpPlot(modelo_rf, \n           main = \"Importância das Variáveis - Random Forest\",\n           pch = 16, # Formato do ponto\n           col = \"blue\") # Cor\n\n\n\n\n\n\n\n# O gráfico é gerado diretamente a partir do objeto do modelo salvo\nplot(modelo_rf, main = \"Erro do Modelo vs. Número de Árvores\")\nlegend(\"topright\", \n       legend = \"Erro OOB\", \n       col = \"blue\", \n       lty = 3)\n\n\n\n\n\n\n\n# Criar um dataframe com os valores reais e previstos\nresultados_rf &lt;- data.frame(\n  Reais = y_test_rf,      # y_test_rf do seu script anterior\n  Previstos = y_pred_rf   # y_pred_rf do seu script anterior\n)\n\n# Gerar o gráfico\nggplot(resultados_rf, aes(x = Reais, y = Previstos)) +\n  geom_point(alpha = 0.6, color = \"blue\") +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(\n    title = \"Random Forest: Valores Previstos vs. Reais\",\n    subtitle = \"A linha vermelha representa a previsão perfeita\",\n    x = \"Volume Real\",\n    y = \"Volume Previsto\"\n  ) +\n  theme_minimal() +\n  coord_fixed() # Garante que a escala dos eixos seja a mesma (1:1)\n\n\n\n\n\n\n\n# 1. Carregar Pacotes \nlibrary(rpart)\nlibrary(rpart.plot)\n\n\n\n# 2. Construir uma ÚNICA Árvore de Decisão\n# Usamos o método \"anova\" para regressão (prever um número contínuo)\narvore_decisao &lt;- rpart(\n  VOLUME ~ .,\n  data = dados_treino_rf,\n  method = \"anova\"\n)\n\n# 3. Gerar o Gráfico da Árvore\n# A função rpart.plot cria uma visualização muito mais informativa e bonita\nrpart.plot(\n  arvore_decisao,\n  type = 4,                   # Estilo do gráfico (existem vários)\n  extra = 101,                # Adiciona informações extras nos nós\n  box.palette = \"BuGn\",       # Paleta de cores para os \"nós\"\n  branch.lty = 3,             # Estilo da linha dos \"galhos\"\n  shadow.col = \"gray\",        # Cor da sombra das caixas\n  main = \"Previsão de Volume\"\n)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Randon Forest</span>"
    ]
  }
]