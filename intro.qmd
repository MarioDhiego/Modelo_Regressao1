
```{=html}
<style>
  body{text-align: justify}
</style>
```


::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::


# Modelo de Regressão Linear 
## Regressão Simples

A Técnica de **Regressão Linear** é uma das mais conhecida e utilizadas na Estatística. È a porta de entrada para diversos modelos preditivos mais sofisticados, já que muitos destes usam conceitos também utilizados na regressão linear. Essencialmente, a regressão linear pode ser utilizada para prever o valor de uma variável quantitativa (**dependente**) em função das outras variáveis (**independentes ou preditoras**).


## Dataset

Para ilustrar a regressão simples, vamos começar com um exemplo em que queremos estudar a relação entre idade (**variável preditora $X_{1}$**) e Salário (**variável dependente Y**) com uma amostra de 80 funcionários do Supermercadp Formosa.


* Local: Supermercado Formosa
* Amostra: 80 pessoas
* ID : Indetidade do Funcionário
* EDUCAÇÃO : Nível Educacional do Funcionário
* CARGO : Cargo do Funcionário
* LOCAL : Local onde Atua o Funcionário
* IDADE : Idade em anos Completos do Funcionário
* TEMPOCASA : Tempo de Casa
* SALARIO : Salário Mensal do Funcionário em R\$

Para ilustrar a regressão simples, vamos começar com um exemplo em que queremos estudar a relação entre idade (**variável preditora $X_{1}$**) e Salário (**variável dependente Y**) com uma amostra de 80 funcionários do Supermercadp Formosa.

Vamos assumir que o salário varia linearmente conforme a idade. Matematicamente, diremos que o salário é uma função linear da idade: salário = $Salário \ = \ \beta_{0}+\beta_{1}*Idade$. Entretanto, sabemos que esta relação não é determinística, isto é, não necessariamente a diferença salarial entre uma pessoa com 30 anos e outra com 31 será $\beta_{1}$. Isso ocorre porque há outros fatores que interferem no salário e não estão incluídos no modelo. Este ruído será representado por um termo de erro do modelo:

$$ Salário = \beta_{0} + \beta_{1} * Idade + erro$$

NO modelo de regressão simples tradicional, o termo de erro tem valor esperado igual a7 zero, e isso implica no salário médio das pessoas com determinada idade, denotado por E(Salário), dado pela parte determinística da equação:


$$ E(Salário) = \beta_{0} + \beta_{1} * Idade$$ 


$\beta_{0} + \beta_{1}$ são parâmetros do modelo e podem ser estimados a partir dos dados da amostra. NO exemplo , usaremos os dados amostrais para estimar esses parâmetros. o Primeiro passo e construir um gráfico de dispersão em que colocamos a idade no eixo x e o salário no eixo y. 


::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::


### Gráfico de Dispersão Geral

O script utilizado para gerar o gráfico de dispersão no R é mostrado a seguir:


```{r Dispersao1, message=FALSE, warning=FALSE}

library(ggplot2)
library(plotly)
library(readxl)

mercado <- read_excel('mercado.xlsx')

Plot2 <- ggplot(mercado, aes(x=IDADE, y= SALARIO))+
  geom_point(size = 2.5, 
             pch = 21, 
             col = 'black',
             fill = 'red')+
  geom_smooth(method="lm", 
              se= TRUE)+
  theme_bw()+
  labs(x="IDADE", 
       y="SALÁRIO", 
       title="Diagrama de Dispersão Geral", 
       subtitle = "Renda Salarial")
ggplotly(Plot2)
```


O gráfico mostra originalmente um ponto muito distante dos demais, no qual é o salário de um dos diretores da Empresa que ganha **R$ 12.465,80**



```{r base2, message=FALSE, warning=FALSE}
library(readxl)

mercado2 <- read_excel('mercado2.xlsx')

```


O script utilizado para gerar o gráfico de dispersão no R sem a observação 69 correspondente a (**60 anos; R$ 12.465,80**) é mostrado a seguir:


::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::


### Gráfico de Dispersão Sem obs: 69º


```{r Dispersao2, message=FALSE, warning=FALSE}

library(ggplot2)
library(plotly)
library(readxl)

mercado2 <- read_excel('mercado2.xlsx')

Plot3 <- ggplot(mercado2, aes(x=IDADE, y= SALARIO))+
  geom_point(size = 2.5, 
             pch = 21, 
             col = 'black',
             fill = 'red')+
  geom_smooth(method="lm", 
              se= TRUE)+
  theme_bw()+
  labs(x="IDADE", 
       y="SALÁRIO", 
       title="Diagrama de Dispersão sem Outliers", 
       subtitle = "Renda Salarial")
ggplotly(Plot3)
```



O gráfico mostra que há uma tendência de crescimento do salário quando a idade aumenta, ilustrado pela reta inclinada, que chamaremos de reta de mínimos quadrados.

::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::


## Reta de Mínimos Quadrados

A seguir, vamos ver como encontrar a reta que estabelece uma relação entre as duas variáveis:

$$ \hat{y} = \hat{\beta_{0}} + \hat{\beta_{1}x}$$

O símbolo "^" em $\beta_{0}$ e $\beta_{1}$ indica que estamos estimando os parâmetros do modelo populacional , já que contaremos apenas com dados amostrais no nosso cálculo.  $\hat{y}$ é o valo previsto do salário médio dos funcionários com idade "x".

O objetivo é obter estimadores $\hat{\beta_{0}}$ e $\hat{\beta_{1}}$, isto é, a reta, que melhor se ajusta aos dados. Como critério de ajuste utilizaremos a "Soma de Quadrados dos Resíduos" (SQR), definida a seguir. O resíduo da7 observação "i" da amostra é a diferença entre o seu valor observado $y_{i}$ e o valor previsto $\hat{y}_{i}$.


$$ SQR = min \sum_{i=1}^{n} (y_{i}-\hat{y}_{i})^{2} = min \sum_{i=1}^{n} e_{i}^{2}$$

em que $e_{i}$ é o resíduo da observação "i".



## Coeficiente de Regressão Linear

Para fazer uma análise de regressão no R, usaremos a função *lm*, do **pacote basic**, e os dados do Supermercado Formosa. A sintaxe para rodar a regressão linear simples é **lm(y~x)**.


### Tabela de Resultado Padrão

Uma das tarefas mais corriqueiras dos analista de dados é a produção de tabelas. Seja para apresentar frequências, estatísticas descritivas (media, mediana, moda, etc), seja para apresentar resultados de modelos de regressão.

Tabela de resultados referente ao modelo de regressão linear simples com n=80.


```{r model1, message=FALSE, warning=FALSE}

Modelo1 = lm(mercado$SALARIO~mercado$IDADE)
summary(Modelo1)
```


Na saída acima podemos ver os estimadores $\hat{\beta_{0}}$ e $\hat{\beta_{1}}$ (estimate), seus erros padrão (Std. Error), a estatística t (t value) e o valor-p do teste de hipótese (Pr(>|t|)).

Os etimadores $\hat{\beta_{0}}$ e $\hat{\beta_{1}}$ possuem um erro padrão que depende de vários fatores, entre eles o tamanho da amostra e o desvio-padrão do erro do modelo. Com esses valores podemos construir uma estimativa intervalar, com determinado nível de confiança, para os parâmetros populacionais desconhecidos $\beta_{0}$ e $\beta_{1}$.


```{r model2, message=FALSE, warning=FALSE}

Modelo2 = lm(mercado2$SALARIO~mercado2$IDADE)
summary(Modelo2)
```


A presença desse outlier tem consequências importantes para o resultado da regressão. Os valores de $\hat{\beta}_{0}$ e $\hat{\beta}_{1}$ mudam de (1818,11; 79,59) para (2165,81; 71,08), respectivamente, o que faz com que os valores previstos também mudem, especialmente nos extremos, isto é, para idades muito baixas e muito altas. Isso significa que esse ponto, além de **outlier**, é um **ponto influente**, isto é, a presença dele muda as estimativas do modelo. 

::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::


### Tabela de Resultados Personalizada

O **pacote gtsummary** fornece uma maneira simples e sofisticada de criar tabelas para apresentar os resultados de modelos de regressão no R. A função **tbl_regression()** pega um objeto de modelo de regressão em R e retorna uma tabela formatada de resultados do modelo de regressão que está pronta para publicação.

Tabela de resultados referentes ao modelo de regressão liear simples sem a observação 69.



```{r tabreg1, message=FALSE, warning=FALSE}
library(magrittr)
library(dplyr)
library(gtsummary)

Modelo2 = lm(mercado2$SALARIO~mercado2$IDADE) 

Modelo2 %>%
tbl_regression() %>%
  add_global_p() %>%
  bold_p(t = 0.05) %>%
  bold_labels() %>%
  italicize_levels() %>% 
  modify_header(label = "**VARIAVEIS**") %>%
  modify_caption("**Tabela 01. Modelo de Regressao Simples do Salário em função da Idade, Formosa - 2023.**")
```





::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::


## Análise de Resíduo

Os Testes de hipóteses anteriores só têm validade se as suposições do modelo estiverem satisfeitas. As suposições são as mesmas do modelo de regressão simples ou multipla: o erro deve ter distribuiçao normal com média 0, variância constante e independentes.

Existem diversos pacotes na linguagem R para fazer a análise dos resíduos, mas será enfatizado apenas os gráficos mai comuns, que podem ser feitos sem a instalação de nehhum pacote adicional.

O script a seguir cria os gráficos para regressão linear simples com n=80.


```{r residuo1, message=FALSE, warning=FALSE}

par(mfrow=c(2,2))
plot(Modelo1)
```


O primeiro gráfico é chamado de (**Residuals vs Fitted**) mostra que a observação 69 tem um resíduo extremamente alto, considerando um outlier da regressão.

O segundo gráfico, na direita superior, é chamado de (**QQ plot**), e é uma alternativa ao histograma para averiguar se há normalidade dos erros. Espera-se que, se a distribuição for normal, os pontos estarão próximos a uma reta. A observação 69 está bem longe dessa reta, colocando em dúvida a suposição de que os erros têm distribuição normal.

O gráfico da esquerda inferior (**Scale-Location**) pode ser utilizado para averiguar se a variância é constante. Quando a
variância é constante, a linha cinza-claro não  apresenta oscilações  significativas ao longo do eixo x.

O gráfico inferior a direita (**Residuals vs Leverage**) nos ajuda a identificar pontos influentes na regressão. Utiliza-se como critério a distância de **Cook**. 

A distância de Cook mede o quanto determinada observação influência o resultado da regressão.

Pontos acima da linha tracejada inferior são considerados influentes, caso da observação 69.

O script a seguir cria os gráficos para regressão linear simples com n=79, ou seja, sem a observação 69, considerada um outlier.


```{r residuo2, message=FALSE, warning=FALSE}

par(mfrow=c(2,2))
plot(Modelo2)
```


::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::


## Normalidade dos Resíduos

```{r normalidade1, message=FALSE, warning=FALSE}

shapiro.test(Modelo2$residuals)
```


::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::

## Independência dos Resíduos

```{r independencia1, message=FALSE, warning=FALSE}
library(car)  

durbinWatsonTest(Modelo2)
```


::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::

## Homocedasticidade dos Resíduos (Breusch-Pagan)

```{r homocedas1, message=FALSE, warning=FALSE}
library(lmtest)  

bptest(Modelo2)
```


::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::



## Previsão

Para fazer a previsão do salário usando a linguagem R, pode-se utilizar a função **predict**. Por exemplo, que estamos interessados prever o salário de um funcionário de 40 anos e outro de 50 anos. 

O script no R é dado a seguir: 


```{r previsao1, message=FALSE, warning=FALSE}
NovaIdade = data.frame(IDADE = c(40,50))

predict(Modelo1, 
        newdata = NovaIdade, 
        interval = "prediction"
        )
```


Pode-se concluir, que um funcionário de 40 anos terá um salário entre **R$ ** e  **R$ ** com 95\% de probabilidade.


::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::



## Regressão Múltipla

Geralmente os modelos de regressão linear simples possuem alto erro padrão e baixo $R^{2}$. Isso porque a variável Y(salário) é explicada por diversos fatores, não só pela idade do funcionário. Portanto, seria interessante utilizar mais de uma variável preditora no modelo para prever o salário com maior precisão.

Quando há mais de uma variável preditora, temos o **Modelo de Reressão Múltipla**. 

Em termos gerais, um modelo de regressão linear múltipla é dado por:


$$ Y_{i} = \beta_{0} + \sum_{j=1}^{k} \beta_{j} X_{ij}+\epsilon_{i}$$

Com adição de variáveis ao modelo, espera-se que o $R^{2}$ aumente consideravelmente. Na verdade, mesmo uma variável irrelevante para o modelo provocará um aumento  (insignificante do $R^{2}$). Desta forma, não é recomendável utilizar o $R^{2}$ para comparar um modelo com um variável preditora e outro modelo com duas preditoras. O modelo com duas preditoras sempre possuirá um $R^{2}$ maior, principalmente se o número de variáveis prediotas for grande em comparação com o tamanho da amostra.


O $R^{2}_{ajustado}$ é uma medida que permite comparar modelos com diferentes tamanhos, pois para cada variável adicionada ao modelo a medida sofre uma penalização.

O $R^{2}_{ajustado}$ de um modelo com mais variáveis só aumentará se essa nova variável ajudar a prever o Y. Caso contrário, o $R^{2}_{ajustado}$ pode diminuir em relação ao modelo sem tal variável.

O modelo de regressão linear múltipla utilizará as variáveis: idade, tempo de casa, educação, cargo e local de trabalho para prever o salário.

A função utilizada para rodar a regressão múltipla é a mesma que usamos para o modelo1 de regressão simples, mas agora colocaremos as variáveis preditoras após o "~" e separadas por "+".


```{r model3, message=FALSE, warning=FALSE}

Modelo3 = lm(mercado2$SALARIO~mercado2$EDUCAÇÃO+mercado2$CARGO+mercado2$LOCAL+mercado2$IDADE+mercado2$TEMPOCASA)
summary(Modelo3)
```

Note que a saída do R para regressão múltipla é bem similar à da simples, A diferença é que agora há várias linhas, uma para cada variável independente, com suas estimativas, erros padrão, estatística t e valor-p.




## Interpretação dos Coeficientes

O primeiro coeficiente, relativo a variável **Educação**, motra uma estimativa da diferença média entre os salários dos funcionários com nível superior e com nível secundário, mantida todas as variáveis constante. O valor estimado é 128,199. Entretanto, o erro padrão dessa estimativa é grande (108,17) e acarreta um valor-p alto = 0.23. Portanto, não conseguimos rejeitar a hipótese de que o coeficiente $\beta_{1}$ é diferente de zero. Desta forma, variável "Educação" não se mostrou significante no modelo, e será retirada.

O segundo o terceiro coeficiente estão relacionados ao **Cargo**.
O valor-p de ambas é muito baixo, menor que 0.0001, indicando fortes evidências estatísticas de que esses coeficientes na população são diferentes de zero.


O quarto coeficiente, relativo a **Local de Trabalho**, assim como o primeiro, não é significante (valor-p = 0.14). Portanto, não há evidências estatísticas ao nível de 95\% de significância de que os salários médios na capital e no interior são diferentes.


O quinto coeficiente, relativo a **Idade**, mostra que o salário médio tem um aumento estimado de 18.68 por ano. Ao nível de significância de 0.05, podemos dizer que há evidências estatísticas de que o coeficiente na população é diferente de zero.


O sexto coeficiente, relativo ao **Tempo de Casa**, indica que a cada ano dicional do funcionário no supermercado há um amento estimado no salário de 75,007 reais. Neste caso, conclui-se também que há evidências de que o coeficiente relativo ao tempo de casa na população $\beta_{6}$, é diferente de zero, pois valor-p < 0.0001.


```{r tabreg2, message=FALSE, warning=FALSE}
library(magrittr)
library(dplyr)
library(gtsummary)

Modelo3 = lm(mercado2$SALARIO~mercado2$EDUCAÇÃO+mercado2$CARGO+mercado2$LOCAL+mercado2$IDADE+mercado2$TEMPOCASA)

Modelo3 %>%
tbl_regression(intercept = TRUE,
               conf.level = NULL) %>%
  modify_header(label = "**VARIAVEIS**") %>%
  modify_caption("**Tabela 01. Modelo de Regressao de Salário e Idade**") %>%
  add_n() %>%
  add_global_p() %>%
  bold_p(t = 0.05) %>%
  bold_labels() %>%
  italicize_levels() %>%
  add_vif(c("aGVIF", "df"))
```



::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::


## Modelo Final

Com base no modelo anterior, verificou-se que, as variáveis: Nível Educacional e Local onde Atua o Funcionário não se mostraram estatisticamente significantes, com isso foram retirada na composição do modelo final. 

Note que o modelo final não fica guardado em nenhum objeto no R. È preciso rodar novamente apenas as variáveis selecionadas.


```{r model4, message=FALSE, warning=FALSE}

Modelo4 = lm(mercado2$SALARIO~+mercado2$CARGO+mercado2$IDADE+mercado2$TEMPOCASA)
summary(Modelo4)
```


Pode-se viasualizar os resultados do modelo final de forma gráfica.


```{r plotreg1, message=FALSE, warning=FALSE}
library(GGally)

ggcoef(Modelo4,
       exclude_intercept = FALSE,
       vline_color = "red",
       vline_linetype = "solid",
       errorbar_color = "blue",
       errorbar_height = 0.5,
       size = 5, 
       shape = 18)

```



::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::

## Comparação dos Modelos

```{r AIC, message=FALSE, warning=FALSE}
library(car)

AIC(Modelo1,Modelo2, Modelo3, Modelo4)
BIC(Modelo1,Modelo2, Modelo3, Modelo4)


```


::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::

## Multicolinearidade

Quando uma variável preditora possui alta correlação com outras variáveis preditoras ou com uma combinação delas, temos um problema de multicolinearidade na regressão.


Quando isso ocorre, as estimativas dos coeficientes apresentam alto erro padrão e geralmente são não significantes, tornando difícil avaliar o efeito de cada preditor no modelo.


Há diversas formas de detectar multicolinearidade, sendo as mais utilizadas a  **Tolerância** e o **VIF(Variance inflation fator)**. Ambas baseiam-se em quanto uma variável  preditora pode ser explicada pela combinação linear das outras variáveis preditoras.

Uma boa medida disto é o $R^{2}$ da regressão em que a variável preditora "j" é explicada por todas as outras variáveis preditora. Um $R^{2}$ alto indica multicolinearidade. 

A Tolerância e o VIF da variável preditora "j" são dados por:

$$ Tolerância = 1- R^{2}_{j}$$

$$ VIF =  \frac{1}{Tolerância} = \frac{1}{1- R^{2}_{j}}$$



O cálculo do VIF na linguagem R pode ser feito utilizando o **pacote car**.



```{r vif1, message=FALSE, warning=FALSE}

library(car)
vif(Modelo3)

```



Vinhos que nenhuma variável possui VIF (ou GVIF) maior que 5, portanto, não temos problema de multicolinearidade no nosso modelo.


::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::



## Machine Learning no R
### Separando a Base em Treino e Teste

Na maioria das vezes, quando existe um conjunto de dados para utilizar na construção de um modelo, precisa-se fazer uma separação entre o que chamamos de **treino** e **teste**. O que costuma-se chamar de base treino, é o conjunto de dados que utilizaremos na construção do modelo.

Porém, para sabermos se o modelo não tem problemas como **overfitting**, precisamos testar o que foi rodado com a base treino, utilizando o que costumamos chamar de base teste.

No R, fazer essa separação é muito simples. Principalmente quando utilizamos o **pacote caret**, provavelmente o mais utilizado em modelagem estatística.


```{r sepBase1, message=FALSE, warning=FALSE}
library(caret)
library(recipes)


Base_Treino <- createDataPartition(mercado$SALARIO, 
                                   p = 0.8, 
                                   list = FALSE)
Treino <- mercado[Base_Treino,]
Teste <- mercado[-Base_Treino,]

Modelo_Final <- lm(mercado$SALARIO~+
                  mercado$CARGO+
                  mercado$IDADE+
                  mercado$TEMPOCASA, data = Treino)

summary(Modelo_Final)

plot(Modelo_Final)

```



::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::


