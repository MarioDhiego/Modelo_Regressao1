
```{=html}
<style>
  body{text-align: justify}
</style>
```


::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::


  
# Regressão Ridge

A regressão de Cume, Cumeeira, em Crista ou chamada **Ridge Regression**, proposta por Hoerl e Kennard (1970a), é um dos vários métodos propostos para remediar os problemas de multicolinearidade, alterando o método de mínimos quadrados para permitir estimadores viesados dos coeficientes de regressão. 

Quando um estimador tem um viés pequeno e é substancialmente mais preciso que o estimador não viesado, este pode ser escolhido desde que tenha grande probabilidade de estar próximo do valor verdadeiro (Hoerl; Kennard 1970b). Assim a probabilidade do estimador de cumeeira   estar próximo do valor verdadeiro   é muito maior que para o estimador não viesado de mínimos quadrados ordinais (NETER et al., 1996).

Uma medida da combinação do efeito do viés e da variação amostral é o valor esperado do quadrado do desvio do estimador   e do valor verdadeiro. Esta medida é chamada de Erro Médio Quadrático (EMQ), e pode ser escrita como,


$$ E(\hat{\beta}-\beta)^{2} = V(\hat{\beta}) + [E(\hat{\beta})-\beta]^{2}$$

Dessa maneira, o EMQ é igual à variância do estimador mais o viés ao quadrado. Note que se o estimador for não viesado, o erro médio quadrático é igual ao estimador da variância. Pelo método de mínimos quadrados, o coeficiente $\beta$  pode ser estimado como,

$$ \hat{\beta} = (X^{'}X)^{-1}X^{'}Y $$

e as estimativas e suas variâncias poderão ser incertas na presença de multicolinearidade. A regressão de cume consiste na adição de coeficientes   a diagonal principal da matriz de correlações $(X^{'}X)^{-1}$ , causando um decréscimo na variância das estimativas. Desta maneira, o estimador de cume de $\beta$ é obtido por,



$$\hat{\beta} = (X^{'}X + k)^{-1}X^{'}Y$$
Sendo $k= diagonal(k_{1},k_{2},...,k_{p}), k_{i} \geq 0$, onde um procedimento bastante usado é  $k=kI$, $k_{i} \geq 0$ . O estimador em cume é na verdade uma família de estimadores, onde $k$ é um valor pequeno que deve ser escolhido a critério do pesquisador. Em geral, aumenta-se gradativamente o valor de $k$ até que os estimadores dos coeficientes tornam-se estáveis, não variam. Se a escolha for $k_{i} = 0$, para todo $i$, tem-se o estimador de Mínimos Quadrados (NETER; WASSERMAN, 1974), (DRAPER; SMITH, 1981) e (ELIAN, 1998). 


Quando os dados possuem traços de multicolinearidade sempre existe um valor para o parâmetro $k$ no qual os estimadores de Regressão em Crista produzem um Quadrado Médio do Erro (QME) menor do que o QME produzido pelos Estimadores de mínimos quadrados ordinários (HOERL; KENNARD, 1970a), (NETER; WASSERMAN, 1974), (DRAPER; SMITH, 1981), (ELIAN, 1998),  


A função estimada pela Regressão em Crista produz predições com novas observações que tendem a serem mais precisas do que as feitas pela função estimada pelo método de mínimos quadrados ordinários, quando as variáveis independentes são correlacionadas e a nova observação segue o mesmo padrão de multicolinearidade, esta precisão na predição de novas observações é favorecida pela Regressão de Cume, especialmente quando a multicolinearidade é forte (NETER; WASSERMAN, 1974), (DRAPER; SMITH, 1981), (ELIAN, 1998). 


::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::



## Métodos pra Determinar $k$

Um valor ideal para o parâmetro $K$, o qual resulta em um menor QME que o obtido pelo Método de Mínimos Quadrados Ordinários (MQO) depende do vetor de parâmetro  desconhecido e da variância do erro também desconhecido (HOERL; KENNARD, 1970a). Conseqüentemente, K precisa ser determinado empiricamente ou obtido dos dados, e não é possível determinar o valor ideal do parâmetro de cume K. Muitos métodos têm sido propostos para obter os valores apropriados, mas não existe um consenso de qual método é o mais adequado. Assim, o parâmetro de ridge $K$ será estimado a partir de dois métodos: Gráfico do Traço de Cume (**Ridge Trace Plot**) e Gráfico do Fator de Inflação da Variância (**Variance Inflation Factor Plot**). 

Um dos obstáculos principais em utilizar a regressão de cume está em escolher um valor de k. O traço de cume é um esboço dos valores de (p-1) coeficientes estimados de regressão de cume padronizados para diferentes valores de $K$, usualmente entre 0 e 1. Feito o traço, pode-se examinar um valor de $K$ onde as estimativas se estabilizam. Hoerl e Kennard (1970b) desenvolveram um gráfico bidimensional do valor de cada coeficiente versus $k$, mostrando como os valores de $\hat{\beta}$ variam em função dos valores de k, ou seja, a partir do gráfico o analista escolhe um valor para K que os coeficientes da regressão tendem a ser mais precisos, que o MQO, quando os dados estão sob o efeito da multicolinearidade. 

Segundo Chagas et al. (2009), o objetivo é escolher um valor de k a partir do qual as estimativas dos parâmetros sejam relativamente estáveis gerando uma série de coeficientes com menor soma dos quadrados do resíduo do que a solução clássica. Assim, na medida em que se aumenta o valor de k, a soma de quadrados dos resíduos também aumentará, sugerindo iniciar com valores pequenos de k e ir aumentando gradativamente até que os coeficientes se estabilizem.

Para Hoerl e Kennard (1970a), o Fator de Inflação da Variância, mostra a variabilidade em função do valor de K, ou seja, à medida que se atribui valores para K que estabilizam os coeficientes de regressão de cume, a variabilidade diminui, removendo a multicolinearidade. O traço do cume pode também ser usado para sugerir variável(s) que pode(m) ser retiradas do modelo. Algumas variáveis cuja estimativa do parâmetro é instável a cada mudança do valor de K ou que decresce para zero são candidatos para anulação.

Atualmente na literatura, existem várias medidas corretivas para suavizar os efeitos provocados pela multicolinearidade, outros métodos são propostos desde simples as mais complexas, tais como, Ampliação do Tamanho da Amostra e Remoção das Variáveis (HAIR et al., 2005), utilização de Modelo de Regressão por Componente Principais (SILVA et al., 2009), Modelo de Regressão por Análise Fatorial (VALENTE et al., 2011), Regressão com Variáveis Latentes (MALHOTRA, 2011) e Regressão via Redes Neurais (RODRIGUES et al., 2010) e (LEAL et al., 2015). 

Para rodar a regressão Ridge na linguagem R, usaremos o **pacote glmnet**.


::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::


## Carregar Dataset2

Segundo Passo é carregar a base de dados, chamada de Volume de Pinus. Para isso é necessário instalar o pacote para leitura de arquivo com extensão do tipo excel(.xlsx) por meio do comando **install.packages("readxl")**.

Posteriormente, ativar o pacote no R com o comando **library(readxl)**. Tendo um detalhe fundamental, que se instala somente um vez o pacote, e se ativa toda vez que for usar.

```{r base1, message=FALSE, warning=FALSE}
library(readxl)

dados <- read_excel("dados_pinus.xlsx")
```


### Variáveis

Para ilustrar a regressão ridge, vamos começar com um exemplo em que queremos estudar a relação entre DAP (**variável preditora $X_{1}$**) e Volume (**variável dependente Y**) com uma amostra de 250 arvores.


* Local: Empresa Duratex Florestal SP
* Amostra: 20 arvores
* IAF : Indice Aerea Foliar
* DAF : Distribuição Angular da Folha
* GAP : 
* IDADE : Idade em Meses da arvore
* DAP : Diâmetro a Altura do Peito (1.30 metros do solo)
* ALTURA : Altura da arvore
* ÁEREA BASAL : Area Basal da arvore


::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::



## Matriz de Correlação

```{r corr1, message=FALSE, warning=FALSE}
library(corrplot)

correl <- cor(dados[, -1])
corrplot(correl, 
         method = "color",         # circle, number, shade
         type = "upper",            # lower, full
         #order = "hclust",
         addCoef.col = "black",
         tl.srt = 90,
         diag = TRUE)
 

```

::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::


## Matriz de Dispersão

```{r dispersao1, message=FALSE, warning=FALSE}
library(psych)
library(GGally)

pairs.panels(dados,
             method = "pearson",
             density = TRUE,  
             ellipses = TRUE,
             smoother = TRUE)
```

::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::


## Regressão Linear

Uma das tarefas mais corriqueiras dos analista de dados é a produção de tabelas. Seja para apresentar frequências, estatísticas descritivas (media, mediana, moda, etc), seja para apresentar resultados de modelos de regressão.

Tabela de resultados referente ao modelo de regressão linear multiplo com n =250.


```{r modelolinear1, message=FALSE, warning=FALSE}

Modelo_mlm <- lm(VOLUME ~ ., data = dados)
summary(Modelo_mlm)


```


### Fator de Inflanção da Variância (vIF)

```{r vif1, message=FALSE, warning=FALSE}

library(car)

vif(Modelo_mlm)  

```


::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::

## Modelo de Regressão Ridge Geral

```{r modeloridge1, message=FALSE, warning=FALSE}
library(caret)
library(glmnet)

#------------------ Modelo Ridge Completo ----------------------------#
set.seed(123)
train_index <- createDataPartition(dados$VOLUME, p = 0.9, list = FALSE)
dados_treino <- dados[train_index, ]
dados_teste <- dados[-train_index, ]

x_train <- model.matrix(VOLUME ~ ., dados_treino)[, -1]  # Remove intercepto
y_train <- dados_treino$VOLUME

x_test <- model.matrix(VOLUME ~ ., dados_teste)[, -1]
y_test <- dados_teste$VOLUME

# Regressão Ridge (Ridge:alpha = 0; LASSO:alpha = 1)
cv_ridge <- cv.glmnet(x_train, 
                      y_train, 
                      alpha = 0, 
                      standardize = TRUE)

# Melhor lambda
best_lambda_ridge <- cv_ridge$lambda.min
cat("Melhor lambda (Ridge):", best_lambda_ridge, "\n")


modelo_ridge <- glmnet(x_train, 
                       y_train,
                       alpha = 0,
                       lambda = best_lambda_ridge,
                       standardize = TRUE)


# Coeficientes do modelo final
coef_ridge <- coef(modelo_ridge)
print(coef_ridge)

# Predição no conjunto de teste
y_pred_ridge <- predict(modelo_ridge, s = best_lambda_ridge, newx = x_test)

#------------------------------------------------------------------------------#
# Medidas Performance

R2_ridge <- cor(y_test, y_pred_ridge)^2
rmse_ridge <- sqrt(mean((y_test - y_pred_ridge)^2))
mae_ridge <- mean(abs(y_test - y_pred_ridge))
mape_ridge <- mean(abs((y_test - y_pred_ridge) / y_test)) * 100


# --- NOVAS MÉTRICAS: AIC e BIC ---
# Predição nos dados de treino para obter o RSS
y_pred_train_ridge <- predict(modelo_ridge, s = best_lambda_ridge, newx = x_train)
RSS <- sum((y_train - y_pred_train_ridge)^2)

# Extrair os graus de liberdade efetivos do modelo
# O objeto glmnet já nos dá isso!
df <- modelo_ridge$df[which(modelo_ridge$lambda == best_lambda_ridge)]

# Número de observações no treino
n <- nrow(dados_treino)

# Cálculo do AIC e BIC
AIC_ridge <- n * log(RSS/n) + 2 * (df + 1) # Adiciona 1 para o intercepto
BIC_ridge <- n * log(RSS/n) + log(n) * (df + 1) # Adiciona 1 para o intercepto


cat("\n--- Performance do Modelo Ridge ---\n")
cat("R² =", R2_ridge, 
    "\nRMSE =", rmse_ridge, 
    "\nMAE =", mae_ridge, 
    "\nMAPE =" , mape_ridge,
    "\nAIC =", AIC_ridge,
    "\nBIC =", BIC_ridge
)
#------------------------------------------------------------------------------#
```



::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::



### Ridge Trace Plot Geral

```{r modeloridge2, message=FALSE, warning=FALSE}

library(caret)
library(glmnet)
library(reshape2)

modelo_ridge_seq <- glmnet(x_train, y_train, alpha = 0, standardize = TRUE)


# Extrair os coeficientes do modelo Ridge
coefs <- as.matrix(modelo_ridge_seq$beta)
lambdas <- modelo_ridge_seq$lambda
df_coef <- as.data.frame(t(coefs))
df_coef$lambda <- log(lambdas)  # log(lambda)

# Transformar para formato longo (long format)
df_long <- melt(df_coef, id.vars = "lambda", 
                variable.name = "Variavel", 
                value.name = "Coeficiente")

#Plot com ggplot2
ggplot(df_long, aes(x = lambda, y = Coeficiente, color = Variavel)) +
  geom_line(size = 1.2, alpha = 0.9) +  # Linhas suaves e grossas
  geom_vline(xintercept = log(best_lambda_ridge), 
             color = "red", linetype = "dashed", size = 1) +
  labs(
    title = "Ridge Trace Plot",
    subtitle = "Modelo Ridge com Todas",
    x = expression(log(lambda)),
    y = "Coeficientes"
  ) +
  scale_color_brewer(palette = "Dark2") +  # Paleta de cores agradável
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 13, color = "gray40"),
    legend.title = element_blank(),
    legend.text = element_text(size = 10),
    legend.position = "right",
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_line(color = "gray85")
  )
```

::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::


### VIF Trace Plot (Geral)


```{r}

library(ggplot2)
library(reshape2)

# ---------------- MATRIZ DE PREDITORES ---------------- #
X <- scale(x_train)

# ---------------- SEQUÊNCIA DE k ---------------- #
k_values <- exp(seq(
  log(1e-4),
  log(10),
  length.out = 100
))

# ---------------- FUNÇÃO VIF RIDGE ---------------- #
ridge_vif <- function(X, k) {
  XtX <- crossprod(X)
  A <- solve(XtX + diag(k, ncol(X)))
  diag(A %*% XtX %*% A)
}

# ---------------- CÁLCULO DOS VIFS ---------------- #
vif_matrix <- sapply(k_values, function(k) ridge_vif(X, k))

# ---------------- ORGANIZAÇÃO DOS DADOS ---------------- #
vif_df <- as.data.frame(t(vif_matrix))
colnames(vif_df) <- colnames(X)
vif_df$k <- k_values

vif_long <- melt(
  vif_df,
  id.vars = "k",
  variable.name = "Variavel",
  value.name = "VIF"
)

# Remove problemas numéricos
vif_long <- vif_long[
  is.finite(vif_long$VIF) & vif_long$VIF > 0,
]

# ---------------- GRÁFICO FINAL (NORMAL) ---------------- #
ggplot(vif_long,
       aes(x = log(k),
           y = VIF,
           color = Variavel)) +
  geom_line(linewidth = 1.2, alpha = 0.9) +
  geom_hline(yintercept = 10,
             linetype = "dashed",
             color = "gray40") +
  geom_vline(xintercept = log(best_lambda_ridge),
             linetype = "dashed",
             color = "red") +
  scale_y_log10() +
  labs(
    title = "VIF Trace Plot",
    subtitle = "Modelo Ridge Completo",
    x = expression(log(k)),
    y = "VIF",
    color = "Variáveis"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "right",
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 12, color = "gray40")
  )

```







::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::



## Modelo de Regressão Ridge (Sem Area Basal)


```{r modeloridge3, message=FALSE, warning=FALSE}

library(readxl)
library(dplyr)
library(caret)
library(glmnet)
library(car)
library(corrplot)
library(ggcorrplot)
library(ggplot2)
library(reshape2)

dados_sem_area <- dados %>% select(-AREA_BASAL)

modelo2_mlm <- lm(VOLUME ~ ., data = dados_sem_area)
summary(modelo2_mlm)
vif(modelo2_mlm)

# Dividir em treino/teste
set.seed(123)
train_index2 <- createDataPartition(dados_sem_area$VOLUME, p = 0.9, list = FALSE)
dados_treino2 <- dados_sem_area[train_index2, ]
dados_teste2 <- dados_sem_area[-train_index2, ]

# Matriz de preditores
x_train2 <- model.matrix(VOLUME ~ ., dados_treino2)[, -1]
y_train2 <- dados_treino2$VOLUME

x_test2 <- model.matrix(VOLUME ~ ., dados_teste2)[, -1]
y_test2 <- dados_teste2$VOLUME

# Ridge com CV sem Área Basal
cv_ridge2 <- cv.glmnet(x_train2, y_train2, alpha = 0, standardize = TRUE)
best_lambda_ridge2 <- cv_ridge2$lambda.min
modelo_ridge2 <- glmnet(x_train2, 
                        y_train2, 
                        alpha = 0, 
                        lambda = best_lambda_ridge2, 
                        standardize = TRUE)

# Predição e avaliação
y_pred_ridge2 <- predict(modelo_ridge2, s = best_lambda_ridge2, newx = x_test2)

#------------------------------------------------------------------------------#
# Medidas Performance
R2_ridge2 <- cor(y_test2, y_pred_ridge2)^2
rmse_ridge2 <- sqrt(mean((y_test2 - y_pred_ridge2)^2))
mae_ridge2 <- mean(abs(y_test2 - y_pred_ridge2))
mape_ridge2 <- mean(abs((y_test2 - y_pred_ridge2) / y_test2)) * 100

# --- NOVAS MÉTRICAS: AIC e BIC ---
# Predição nos dados de treino para obter o RSS
y_pred_train_ridge2 <- predict(modelo_ridge2, s = best_lambda_ridge, newx = x_train2)
RSS <- sum((y_train2 - y_pred_train_ridge2)^2)

# Extrair os graus de liberdade efetivos do modelo
# O objeto glmnet já nos dá isso!
df <- modelo_ridge2$df[which(modelo_ridge2$lambda == best_lambda_ridge2)]

# Número de observações no treino
n <- nrow(dados_treino)

# Cálculo do AIC e BIC
AIC_ridge2 <- n * log(RSS/n) + 2 * (df + 1) # Adiciona 1 para o intercepto
BIC_ridge2 <- n * log(RSS/n) + log(n) * (df + 1) # Adiciona 1 para o intercepto


cat("\n--- Performance do Modelo Ridge(sem Area Basal) ---\n")
cat("R² =", R2_ridge2, 
    "\nRMSE =", rmse_ridge2, 
    "\nMAE =", mae_ridge2,
    "\nMAPE =" , mape_ridge2,
    "\nAIC =", AIC_ridge2,
    "\nBIC =", BIC_ridge2
    )
```


::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::


### Ridge Trace Plot (Sem Area Basal)

```{r modeloridge4, message=FALSE, warning=FALSE}

library(caret)
library(glmnet)
library(reshape2)


modelo_ridge2 <- glmnet(x_train2, y_train2, alpha = 0, standardize = TRUE)

# Extrai os coeficientes
coefs <- as.matrix(modelo_ridge2$beta)
lambdas <- modelo_ridge2$lambda
df_coef <- as.data.frame(t(coefs))
df_coef$lambda <- log(lambdas)  # log(lambda)
df_long <- melt(df_coef, id.vars = "lambda", variable.name = "Variavel", value.name = "Coeficiente")

# Gráfico customizado com ggplot2
ggplot(df_long, aes(x = lambda, y = Coeficiente, color = Variavel)) +
  geom_line(size = 1.2, alpha = 0.9) +  # Linhas mais espessas e levemente transparentes
  geom_vline(xintercept = log(best_lambda_ridge2), 
             color = "red", linetype = "dashed", size = 1) +
  labs(
    title = "Ridge Trace Plot",
    subtitle = "Modelo Ridge (Sem Área Basal)",
    x = expression(log(lambda)),
    y = "Coeficientes dos preditores"
  ) +
  scale_color_brewer(palette = "Dark2") +  # Paleta de cores suave
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 13, color = "gray40"),
    legend.title = element_blank(),
    legend.text = element_text(size = 11),
    legend.position = "right",
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_line(color = "gray85")
  )
```


::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::





## Modelo de Regressão Ridge (Sem DAP)


```{r modeloridge5, message=FALSE, warning=FALSE}

library(readxl)
library(dplyr)
library(caret)
library(glmnet)
library(car)
library(corrplot)
library(ggcorrplot)
library(ggplot2)
library(reshape2)

dados_sem_dap <- dados %>% select(-DAP)

modelo3_mlm <- lm(VOLUME ~ ., data = dados_sem_dap)
summary(modelo3_mlm)
vif(modelo3_mlm)


# Dividir dados em treino e teste
set.seed(123)
train_idx_dap <- createDataPartition(dados_sem_dap$VOLUME, p = 0.9, list = FALSE)
dados_treino_dap <- dados_sem_dap[train_idx_dap, ]
dados_teste_dap  <- dados_sem_dap[-train_idx_dap, ]

x_train_dap <- model.matrix(VOLUME ~ ., dados_treino_dap)[, -1]
y_train_dap <- dados_treino_dap$VOLUME

x_test_dap <- model.matrix(VOLUME ~ ., dados_teste_dap)[, -1]
y_test_dap <- dados_teste_dap$VOLUME

cv_ridge_dap <- cv.glmnet(x_train_dap, y_train_dap, alpha = 0, standardize = TRUE)
best_lambda_dap <- cv_ridge_dap$lambda.min
cat("Melhor lambda (sem DAP):", best_lambda_dap, "\n")

modelo_ridge_dap <- glmnet(x_train_dap, y_train_dap,
                           alpha = 0,
                           lambda = best_lambda_dap,
                           standardize = TRUE)
coef(modelo_ridge_dap)

# Predição
y_pred_dap <- predict(modelo_ridge_dap, s = best_lambda_dap, newx = x_test_dap)

# Métricas de desempenho
R2_dap <- cor(y_test_dap, y_pred_dap)^2
rmse_dap <- sqrt(mean((y_test_dap - y_pred_dap)^2))
mae_dap <- mean(abs(y_test_dap - y_pred_dap))
mape_dap <- mean(abs((y_test_dap - y_pred_dap) / y_test2)) * 100


cat("\nModelo Ridge SEM DAP:\n")
cat("R² =", R2_dap, 
    "\nRMSE =", rmse_dap, 
    "\nMAE =", mae_dap,
    "\nMAPE =", mape_dap
    )
```



::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::


### Ridge Trace Plot (Sem DAP)


```{r ridgeplot2, message=FALSE, warning=FALSE}

### Ridge Trace Plot (Sem DAP)

library(glmnet)
library(reshape2)
library(ggplot2)

# Ajuste do caminho completo do Ridge (sem fixar lambda)
modelo_ridge_path_dap <- glmnet(
  x_train_dap,
  y_train_dap,
  alpha = 0,
  standardize = TRUE
)

# Extração dos coeficientes
coefs_dap <- as.matrix(modelo_ridge_path_dap$beta)
lambdas_dap <- modelo_ridge_path_dap$lambda

df_coef_dap <- as.data.frame(t(coefs_dap))
df_coef_dap$lambda <- log(lambdas_dap)

df_long_dap <- melt(
  df_coef_dap,
  id.vars = "lambda",
  variable.name = "Variavel",
  value.name = "Coeficiente"
)

# Gráfico Ridge Trace Plot customizado
ggplot(df_long_dap, aes(x = lambda, y = Coeficiente, color = Variavel)) +
  geom_line(size = 1.2, alpha = 0.9) +
  geom_vline(
    xintercept = log(best_lambda_dap),
    color = "red",
    linetype = "dashed",
    size = 1
  ) +
  labs(
    title = "Ridge Trace Plot",
    subtitle = "Modelo Ridge (Sem DAP)",
    x = expression(log(lambda)),
    y = "Coeficientes dos preditores"
  ) +
  scale_color_brewer(palette = "Dark2") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 13, color = "gray40"),
    legend.title = element_blank(),
    legend.text = element_text(size = 11),
    legend.position = "right",
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_line(color = "gray85")
  )
```


::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::


## Modelo de Regressão Ridge (Sem DAP + Area Basal)


```{r modeloridge6, message=FALSE, warning=FALSE}

library(readxl)
library(dplyr)
library(caret)
library(glmnet)
library(car)
library(corrplot)
library(ggcorrplot)
library(ggplot2)
library(reshape2)

dados_sem_dap_ab <- dados %>% select(-DAP, -AREA_BASAL)

# Dividir em treino e teste
set.seed(123)
train_idx_dap_ab <- createDataPartition(dados_sem_dap_ab$VOLUME, p = 0.7, list = FALSE)
dados_treino_dap_ab <- dados_sem_dap_ab[train_idx_dap_ab, ]
dados_teste_dap_ab  <- dados_sem_dap_ab[-train_idx_dap_ab, ]


x_train_dap_ab <- model.matrix(VOLUME ~ ., dados_treino_dap_ab)[, -1]
y_train_dap_ab <- dados_treino_dap_ab$VOLUME

x_test_dap_ab <- model.matrix(VOLUME ~ ., dados_teste_dap_ab)[, -1]
y_test_dap_ab <- dados_teste_dap_ab$VOLUME

cv_ridge_dap_ab <- cv.glmnet(x_train_dap_ab, y_train_dap_ab, alpha = 0, standardize = TRUE)
best_lambda_dap_ab <- cv_ridge_dap_ab$lambda.min
cat("Melhor lambda (sem DAP e Área Basal):", best_lambda_dap_ab, "\n")

modelo_ridge_dap_ab <- glmnet(x_train_dap_ab, y_train_dap_ab,
                              alpha = 0,
                              lambda = best_lambda_dap_ab,
                              standardize = TRUE)


y_pred_dap_ab <- predict(modelo_ridge_dap_ab, s = best_lambda_dap_ab, newx = x_test_dap_ab)

R2_dap_ab <- cor(y_test_dap_ab, y_pred_dap_ab)^2
rmse_dap_ab <- sqrt(mean((y_test_dap_ab - y_pred_dap_ab)^2))
mae_dap_ab <- mean(abs(y_test_dap_ab - y_pred_dap_ab))
mape_dap_ab <- mean(abs((y_test_dap_ab - y_pred_dap_ab) / y_test_dap_ab)) * 100



cat("\nModelo Ridge SEM DAP e SEM Área Basal:\n")
cat("R²:", R2_dap_ab, 
    "\nRMSE:", rmse_dap_ab, 
    "\nMAE:", mae_dap_ab,
    "\nMAPE =", mape_dap_ab
    )

```


::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::



### Ridge Trace Plot (Sem DAP + Area Basal)



```{r ridgeplot1, message=FALSE, warning=FALSE}

### Ridge Trace Plot (Sem DAP e Área Basal)

library(glmnet)
library(reshape2)
library(ggplot2)

# Ajuste do caminho completo do Ridge
modelo_ridge_dap_ab <- glmnet(
  x_train_dap_ab,
  y_train_dap_ab,
  alpha = 0,
  standardize = TRUE
)

# Extração dos coeficientes
coefs_dap_ab <- as.matrix(modelo_ridge_dap_ab$beta)
lambdas_dap_ab <- modelo_ridge_dap_ab$lambda

df_coef_dap_ab <- as.data.frame(t(coefs_dap_ab))
df_coef_dap_ab$lambda <- log(lambdas_dap_ab)  # log(lambda)

df_long_dap_ab <- melt(
  df_coef_dap_ab,
  id.vars = "lambda",
  variable.name = "Variavel",
  value.name = "Coeficiente"
)

# Gráfico Ridge Trace Plot customizado
ggplot(df_long_dap_ab, aes(x = lambda, y = Coeficiente, color = Variavel)) +
  geom_line(size = 1.2, alpha = 0.9) +
  geom_vline(
    xintercept = log(best_lambda_dap_ab),
    color = "red",
    linetype = "dashed",
    size = 1
  ) +
  labs(
    title = "Ridge Trace Plot",
    subtitle = "Modelo Ridge (Sem DAP e Área Basal)",
    x = expression(log(lambda)),
    y = "Coeficientes dos preditores"
  ) +
  scale_color_brewer(palette = "Dark2") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 13, color = "gray40"),
    legend.title = element_blank(),
    legend.text = element_text(size = 11),
    legend.position = "right",
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_line(color = "gray85")
  )
```

::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::



### VIF Trace Plot (Sem DAP + Area Basal)




```{r}

library(ggplot2)
library(reshape2)

# Matriz de preditores padronizada
X <- scale(x_train_dap_ab)
p <- ncol(X)

# Sequência LOGARÍTMICA de k (robusta)
k_values <- exp(seq(
  log(1e-4),   # k mínimo
  log(10),     # k máximo
  length.out = 100
))

# Função VIF Ridge estável
ridge_vif <- function(X, k) {
  XtX <- crossprod(X)
  A <- solve(XtX + diag(k, ncol(X)))
  vif <- diag(A %*% XtX %*% A)
  return(vif)
}

# Calcular VIFs
vif_matrix <- sapply(k_values, function(k) ridge_vif(X, k))

# Organizar dados
vif_df <- as.data.frame(t(vif_matrix))
colnames(vif_df) <- colnames(X)
vif_df$k <- k_values

vif_long <- melt(
  vif_df,
  id.vars = "k",
  variable.name = "Variavel",
  value.name = "VIF"
)

# Remover lixo numérico
vif_long <- vif_long[
  is.finite(vif_long$VIF) & vif_long$VIF > 0,
]

# GRÁFICO FINAL (AGORA APARECE)
ggplot(vif_long, aes(
  x = log(k),
  y = VIF,
  color = Variavel,
  group = Variavel
)) +
  geom_line(linewidth = 2, alpha = 0.9) +
  scale_y_log10() +
  geom_hline(yintercept = 10, linetype = "dashed", color = "gray40") +
  geom_vline(
    xintercept = log(best_lambda_dap_ab),
    linetype = "dashed",
    color = "red"
  ) +
  labs(
    title = "VIF em função de k",
    subtitle = "",
    x = expression(log(k)),
    y = "VIF Ridge (escala log)"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "right",
    legend.title = element_blank()
  )

```






::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::


## Modelo Elastic Net (Geral)

```{r modeloelasticnet1, message=FALSE, warning=FALSE}

library(caret)
library(glmnet)

dados_elastic_net <- dados %>% select(-AREA_BASAL)

dados_en <- na.omit(dados_elastic_net) 
set.seed(123)
train_index_en <- createDataPartition(dados_en$VOLUME, p = 0.9, list = FALSE)
dados_treino_en <- dados_en[train_index_en, ]
dados_teste_en <- dados_en[-train_index_en, ]


# 3. Configurar o Controle de Treinamento (Validação Cruzada)
# Usaremos validação cruzada de 10 folds para encontrar os melhores parâmetros
train_control <- trainControl(method = "cv", number = 10)


# 4. Definir a Grade de Parâmetros para Testar
# O caret irá testar diferentes valores de 'alpha' e 'lambda'
# Vamos testar 11 valores de alpha, de 0 (Ridge) a 1 (Lasso)
# Para cada alpha, o caret testará automaticamente vários lambdas.
tune_grid <- expand.grid(
  alpha = seq(0, 1, length = 11),
  lambda = seq(0.001, 0.2, length = 20) # Um grid de lambdas para testar
)

# 5. Treinar o Modelo Elastic Net
# O caret vai fazer todo o trabalho de testar as combinações da grade
# e encontrar a melhor. Isso pode levar um minuto.
cat("Iniciando a otimização do Elastic Net...\n")
set.seed(123)
modelo_en <- train(
  VOLUME ~ .,
  data = dados_treino_en,
  method = "glmnet",         # Especifica o uso de Ridge, Lasso, Elastic Net
  trControl = train_control, # Aplica a validação cruzada
  tuneGrid = tune_grid,      # Fornece a grade de parâmetros para testar
  preProcess = c("center", "scale") # Boa prática: centralizar e escalar os dados
)

# 6. Ver os Melhores Parâmetros Encontrados
cat("\n--- Melhor Combinação Encontrada pelo Caret ---\n")
print(modelo_en$bestTune)


# 7. Avaliar o Modelo Final no Conjunto de Teste
y_pred_en <- predict(modelo_en, newdata = dados_teste_en)
R2_en <- cor(dados_teste_en$VOLUME, y_pred_en)^2
rmse_en <- sqrt(mean((dados_teste_en$VOLUME - y_pred_en)^2))
mae_en <- mean(abs(dados_teste_en$VOLUME - y_pred_en))
mape_en <- mean(abs((dados_teste_en$VOLUME - y_pred_en) / dados_teste_en$VOLUME)) * 100

# 8. Apresentar os Resultados Finais
cat("\n--- Performance do Modelo Elastic Net Otimizado ---\n")
cat("Melhor alpha:", modelo_en$bestTune$alpha, "\n")
cat("Melhor lambda:", modelo_en$bestTune$lambda, "\n")
cat("----------------------------------------\n")
cat("R² =", R2_en, "\n")
cat("RMSE =", rmse_en, "\n")
cat("MAE =", mae_en, "\n")
cat("MAPE =", mape_en, "%\n")

```




::: progress
::: {.progress-bar style="width: 100%;"}
:::
:::

















